{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d273c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import cv2\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55bc3a08",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "data = []\n",
    "with open('adek20k/ade20k_train_localized_narratives.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Create DataFrame with only 'caption' and 'image_id' columns\n",
    "df_adk1 = pd.DataFrame([\n",
    "    {\"target_text\": item[\"caption\"], \"img_path\": item[\"image_id\"]}\n",
    "    for item in data\n",
    "])\n",
    "\n",
    "df_adk1['img_path'] = df_adk1['img_path'].apply(lambda x: 'adek20k/training/'+x+'.jpg')\n",
    "\n",
    "data = []\n",
    "with open('adek20k/ade20k_validation_captions.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Create DataFrame with only 'caption' and 'image_id' columns\n",
    "df_adk2 = pd.DataFrame([\n",
    "    {\"target_text\": item[\"caption\"], \"img_path\": item[\"image_id\"]}\n",
    "    for item in data\n",
    "])\n",
    "\n",
    "df_adk2['img_path'] = df_adk2['img_path'].apply(lambda x: 'adek20k/validation/'+x+'.jpg')\n",
    "\n",
    "df_adk = pd.concat([df_adk1,df_adk2]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "data = []\n",
    "with open('flickr/flickr30k_train_localized_narratives.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Create DataFrame with only 'caption' and 'image_id' columns\n",
    "df_flickr = pd.DataFrame([\n",
    "    {\"target_text\": item[\"caption\"], \"img_path\": item[\"image_id\"]}\n",
    "    for item in data\n",
    "])\n",
    "\n",
    "df_flickr['img_path'] = df_flickr['img_path'].apply(lambda x: 'flickr/flickr30k_images/'+x+'.jpg')\n",
    "\n",
    "\n",
    "question_pool = [\n",
    "    \"What is present in the image?\",\n",
    "    \"How would you describe the background setting?\",\n",
    "    \"What is the central subject or point of focus?\",\n",
    "    \"What is happening in the image?\",\n",
    "    \"Where might this photo have been taken?\",\n",
    "    \"Describe the image.\",\n",
    "    \"Are there people in the picture?\",\n",
    "    \"Is there an animal in the picture?\",\n",
    "    \"What objects are visible in the image?\",\n",
    "    \"What activity or event is occurring in the image?\",\n",
    "    \"Can you describe the colors present in the image?\",\n",
    "    \"What do you think the mood or atmosphere of the image is?\",\n",
    "    \"What draws your attention most in the image?\",\n",
    "    \"Does the image appear to be candid or posed?\",\n",
    "    \"Is there any text or signage visible in the image?\",\n",
    "    \"Are there any buildings or structures visible?\",\n",
    "    \"Does the image convey any emotions or feelings?\",\n",
    "    \"Are there any artistic or stylistic elements in the photo?\",\n",
    "    \"If you could give this image a title, what would it be?\",\n",
    "    'What is unique about the image?',\n",
    "    'What can you tell?',\n",
    "]\n",
    "\n",
    "df_flickr['input_text'] = None\n",
    "\n",
    "# Assign questions per img_path group\n",
    "for img_path, group in df_flickr.groupby('img_path'):\n",
    "    indices = group.index.tolist()\n",
    "    available_questions = question_pool.copy()\n",
    "    np.random.shuffle(available_questions)\n",
    "    \n",
    "    # If more rows than questions, cycle through questions\n",
    "    while len(available_questions) < len(indices):\n",
    "        extra = question_pool.copy()\n",
    "        np.random.shuffle(extra)\n",
    "        available_questions += extra\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        df_flickr.at[idx, 'input_text'] = available_questions[i]\n",
    "\n",
    "\n",
    "df_adk['input_text'] = None\n",
    "\n",
    "# Assign questions per img_path group\n",
    "for img_path, group in df_adk.groupby('img_path'):\n",
    "    indices = group.index.tolist()\n",
    "    available_questions = question_pool.copy()\n",
    "    np.random.shuffle(available_questions)\n",
    "    \n",
    "    # If more rows than questions, cycle through questions\n",
    "    while len(available_questions) < len(indices):\n",
    "        extra = question_pool.copy()\n",
    "        np.random.shuffle(extra)\n",
    "        available_questions += extra\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        df_adk.at[idx, 'input_text'] = available_questions[i]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c3601cf",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Step 1: Load the raw JSON structure\n",
    "with open('GQA/questions1.2/testdev_all_questions.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df1 = pd.DataFrame(data).T[['question','fullAnswer','imageId']].reset_index(drop=True)\n",
    "df1 = df1.rename(columns={'question':'input_text','fullAnswer':'target_text','imageId':'img_path'})\n",
    "df1['img_path'] = df1['img_path'].apply(lambda x: 'GQA/images/'+x+'.jpg')\n",
    "\n",
    "# Step 1: Load the raw JSON structure\n",
    "with open('GQA/questions1.2/testdev_balanced_questions.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df2 = pd.DataFrame(data).T[['question','fullAnswer','imageId']].reset_index(drop=True)\n",
    "df2 = df2.rename(columns={'question':'input_text','fullAnswer':'target_text','imageId':'img_path'})\n",
    "df2['img_path'] = df2['img_path'].apply(lambda x: 'GQA/images/'+x+'.jpg')\n",
    "\n",
    "\n",
    "# Step 1: Load the raw JSON structure\n",
    "with open('GQA/questions1.2/val_balanced_questions.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df3 = pd.DataFrame(data).T[['question','fullAnswer','imageId']].reset_index(drop=True)\n",
    "df3 = df3.rename(columns={'question':'input_text','fullAnswer':'target_text','imageId':'img_path'})\n",
    "df3['img_path'] = df3['img_path'].apply(lambda x: 'GQA/images/'+x+'.jpg')\n",
    "\n",
    "df = pd.concat([df_adk,df_flickr]).dropna().reset_index(drop=True)  # ,df1,df2,df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f61cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the raw JSON structure\n",
    "with open('GQA/questions1.2/val_balanced_questions.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data).T[['question','fullAnswer','imageId']].reset_index(drop=True)\n",
    "df = df.rename(columns={'question':'input_text','fullAnswer':'target_text','imageId':'img_path'})\n",
    "df['img_path'] = df['img_path'].apply(lambda x: 'GQA/images/'+x+'.jpg')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6457724",
   "metadata": {},
   "source": [
    "# Cleaning !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596d41ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Text cleaning ------------------------------------------------------\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove HTML, normalize whitespace, preserve punctuation/numbers/casing.\n",
    "    \"\"\"\n",
    "    # text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9.,!?]+', ' ', text)\n",
    "    text = text.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "df['target_text'] = df['target_text'].apply(lambda x: clean_text(x))\n",
    "df['input_text'] = df['input_text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79890b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine summary + dialogue and split on whitespace\n",
    "raw_lens = [\n",
    "    len(f\"{s} {d}\".split()) \n",
    "    for s, d in tqdm(zip(df['target_text'], df['input_text']), total=len(df))\n",
    "]\n",
    "\n",
    "lens = np.array(raw_lens)\n",
    "\n",
    "# Print summary stats\n",
    "def pct(x): return np.percentile(lens, x)\n",
    "\n",
    "print(f\"Total examples    : {len(lens):,}\")\n",
    "print(f\"Min / Max words   : {lens.min()} / {lens.max()}\")\n",
    "print(f\"Mean ± std        : {lens.mean():.1f} ± {lens.std():.1f}\")\n",
    "print(\"--- Percentiles (word count per raw text pair) ---\")\n",
    "for p in [50, 90, 95, 98, 99]:\n",
    "    print(f\"{p:>3}% : {pct(p):.0f} words\")\n",
    "\n",
    "del(raw_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b1dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = int(pct(99))\n",
    "\n",
    "text_pairs = []\n",
    "\n",
    "for img,i,j in zip(df.img_path,df.input_text,df.target_text):\n",
    "    try:\n",
    "        if len(i.split(\" \")+j.split(\" \")) < MAX_LEN: \n",
    "            text_pairs.append((img,i,j))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "text_pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ecbd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f33ddf",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "# Tokenizing Pre-Trained "
   ]
  },
  {
   "cell_type": "raw",
   "id": "7fa5c8d7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# -------------------- setup --------------------\n",
    "from transformers import AlbertTokenizer, TFAlbertModel       # or AlbertModel for PyTorch\n",
    "import tensorflow as tf                                      # swap for torch if you prefer\n",
    "import numpy as np, itertools\n",
    "\n",
    "model_name = \"albert-base-v2\"\n",
    "tok   = AlbertTokenizer.from_pretrained(model_name)\n",
    "model = TFAlbertModel.from_pretrained(model_name)\n",
    "\n",
    "# 1️⃣  Grab ONLY the 128-d word table  (shape: [vocab, 128])\n",
    "E = model.get_input_embeddings().weights[0]                  # Tensor\n",
    "\n",
    "# -------------------- helper functions --------------------\n",
    "def embed_word(word: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns the mean of sub-word vectors if BPE splits the string.\n",
    "    \"\"\"\n",
    "    ids  = tok(word, add_special_tokens=False)[\"input_ids\"]  # list[int]\n",
    "    vecs = tf.gather(E, ids)                                 # (n_sub, 128)\n",
    "    return tf.reduce_mean(vecs, axis=0).numpy()              # (128,)\n",
    "\n",
    "def cos(a, b):\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "# -------------------- compute similarities ----------------\n",
    "words = [\"king\", \"queen\", \"rich\", \"poor\", \"river\"]\n",
    "vecs  = {w: embed_word(w) for w in words}\n",
    "\n",
    "print(\"Cosine similarities in 128-d ALBERT space\\n\")\n",
    "for a, b in itertools.combinations(words, 2):\n",
    "    print(f\"{a:>5s} ↔ {b:<5s}: {cos(vecs[a], vecs[b]): .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb52d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizer, TFAlbertModel\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "albert = TFAlbertModel.from_pretrained(\"albert-base-v2\")\n",
    "token_model = albert.get_input_embeddings()   \n",
    "token_model.trainable = False  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7805ad2e",
   "metadata": {},
   "source": [
    "# IMAGE EMBEDDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a9978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV3Small\n",
    "import tensorflow as tf\n",
    "\n",
    "# Desired network input size\n",
    "IMG_SHAPE = (224, 224, 3)\n",
    "\n",
    "def build_image_encoder():\n",
    "    base = MobileNetV3Small(input_shape=IMG_SHAPE,\n",
    "                            weights=None,\n",
    "                            include_top=False,\n",
    "                            pooling=None)\n",
    "    base.load_weights(\"weights_mobilenet_v3_small_224_1.0_float_no_top_v2.h5\")\n",
    "    base.trainable = False                    # keep it frozen\n",
    "\n",
    "    img_input  = tf.keras.Input(shape=IMG_SHAPE, dtype=tf.float32, name=\"image\")\n",
    "    feat_map   = base(img_input, training=False)        # (B, H, W, C)\n",
    "\n",
    "    # Automatically infer H, W, C from static shape\n",
    "    h, w, c = feat_map.shape[1], feat_map.shape[2], feat_map.shape[3]\n",
    "    reshaped = tf.keras.layers.Reshape((1,h*w*c))(feat_map)  # (B, 1,H*W*C)\n",
    "\n",
    "    return tf.keras.Model(img_input, reshaped, name=\"img_encoder\")                                           # (224,224,3)\n",
    "\n",
    "model_img = build_image_encoder()\n",
    "\n",
    "def decode_and_resize(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SHAPE[:2])\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img\n",
    "\n",
    "paths = tf.constant([\"vqa/rename/350623.jpg\", \"vqa/rename/56205.jpg\"])\n",
    "images = tf.map_fn(decode_and_resize, paths, fn_output_signature=tf.float32)\n",
    "v3_output = model_img.predict(images)\n",
    "v3_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69758d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_id = tokenizer.convert_tokens_to_ids('<pad>')\n",
    "unk_id = tokenizer.convert_tokens_to_ids('<unk>')\n",
    "bos_id = tokenizer.convert_tokens_to_ids('[CLS]')\n",
    "eos_id = tokenizer.convert_tokens_to_ids('[MASK]')\n",
    "sep_id = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "img_token_id = tokenizer.convert_tokens_to_ids('$')\n",
    "\n",
    "def custom_tokenize(input_a, input_b=None):\n",
    "    tokens = ['$'] * v3_output.shape[1] + ['[CLS]'] + tokenizer.tokenize(input_a) + ['[SEP]']\n",
    "    \n",
    "    if input_b:\n",
    "        tokens += tokenizer.tokenize(input_b) + ['[MASK]']\n",
    "    \n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    return token_ids\n",
    "\n",
    "tokens_single = custom_tokenize(\"A man\")\n",
    "tokens_pair = custom_tokenize(\"A man\", \"on moon\")\n",
    "print(tokens_single)\n",
    "print(tokens_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1772e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full vocab: token → id\n",
    "vocab_dict = tokenizer.get_vocab()\n",
    "\n",
    "# To sort it by ID (optional, for better readability)\n",
    "sorted_vocab = dict(sorted(vocab_dict.items(), key=lambda item: item[1]))\n",
    "sorted_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7754e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_pairs = [custom_tokenize(x,y) for i,x,y in text_pairs]\n",
    "\n",
    "lengths = [len(sublist) for sublist in tokenized_pairs]\n",
    "percentiles = [80, 90, 95, 99]\n",
    "\n",
    "results = {p: np.percentile(lengths, p) for p in percentiles}\n",
    "del(tokenized_pairs)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92feb3",
   "metadata": {},
   "source": [
    "# Tokenizing , Embedding and Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13d8a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = int(np.percentile(lengths, 99))\n",
    "\n",
    "def encode_pair(input_a, input_b=None):\n",
    "    tokens = ['$'] * v3_output.shape[1] + ['[CLS]'] + tokenizer.tokenize(input_a) + ['[SEP]']\n",
    "    if input_b:\n",
    "        tokens += tokenizer.tokenize(input_b) + ['[MASK]']\n",
    "    \n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # Pad to MAX_LEN\n",
    "    if len(token_ids) < MAX_LEN:\n",
    "        token_ids += [pad_id] * (MAX_LEN - len(token_ids))\n",
    "    else:\n",
    "        token_ids = token_ids[:MAX_LEN]\n",
    "\n",
    "    return np.array(token_ids, dtype=np.int32)\n",
    "\n",
    "def encode_example(img_path, text: str, summary: str):\n",
    "    ids = encode_pair(text, summary)  # shape = [MAX_LEN]\n",
    "    labels = np.concatenate([ids[1:], [pad_id]])  # shifted right\n",
    "\n",
    "    # find SEP\n",
    "    sep_idxs = np.where(labels == sep_id)[0]\n",
    "    sep_pos = int(sep_idxs[0]) if sep_idxs.size else len(ids)\n",
    "\n",
    "    # build base mask: 1 only for positions > sep_pos AND not PAD\n",
    "    positions = np.arange(len(labels))\n",
    "    loss_mask = (positions > sep_pos).astype(np.float32) * (labels != pad_id).astype(np.float32)\n",
    "\n",
    "    return img_path, ids, labels, loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3251901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'flickr/flickr30k_images/1000092795.jpg'\n",
    "text = 'hello'\n",
    "summary = 'good morning'\n",
    "\n",
    "img_path, input_ids, label_ids, loss_mask = encode_example(img_path, text, summary)\n",
    "\n",
    "print(\"  Input IDs :\", input_ids)\n",
    "print(\"\\n Label IDs :\", label_ids)\n",
    "print(\"\\n Loss Mask :\", loss_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a210bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 0. do *all* tokenisation once ──────────────────────────\n",
    "triples = [encode_example(img, t, s) for (img, t, s) in text_pairs]   # Python loop, done **once**\n",
    "img_path, ids, labels, masks = map(lambda k: tf.constant(np.stack(k, 0)),zip(*triples))                      # shapes [N, MAX_LEN]\n",
    "\n",
    "# ── 1. build the purely-TF dataset ─────────────────────────\n",
    "ds = (\n",
    "    tf.data.Dataset.from_tensor_slices({'img_path':img_path,\"input_ids\": ids, \"labels\": labels, \"loss_mask\": masks})\n",
    "    .shuffle(len(text_pairs))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "del(triples)\n",
    "del(img_path)\n",
    "del(ids)\n",
    "del(labels)\n",
    "del(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b877d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()           # indices 0 … V\n",
    "\n",
    "id_to_token = {idx: tok for tok, idx in vocab.items()}\n",
    "\n",
    "# 2) Decoder: drop PADs\n",
    "def decode_token_ids(token_ids: list[int]) -> str:\n",
    "    tokens = []\n",
    "    for tid in token_ids:\n",
    "        if tid == pad_id:\n",
    "            continue\n",
    "        tok = id_to_token.get(tid, '?')\n",
    "        if tok == '$':\n",
    "            continue  # Skip the '$' symbol\n",
    "        if tok.startswith('Ġ'):\n",
    "            tok = tok[1:]  # Remove the space prefix indicator\n",
    "            tokens.append(' ' + tok)\n",
    "        else:\n",
    "            tokens.append(tok)\n",
    "    return ' '.join(tokens).strip()\n",
    "\n",
    "# 3) Inspect one batch from your TF dataset\n",
    "for batch in ds.take(10):\n",
    "    pth = batch['img_path'].numpy()\n",
    "    input_ids = batch['input_ids'].numpy()  # shape (batch, MAX_LEN)\n",
    "    labels    = batch['labels'].numpy()\n",
    "\n",
    "    for i, (pth, ids_row, lbl_row) in enumerate(zip(pth, input_ids, labels), start=1):\n",
    "        print(f\"\\n🟢 Sample {i}\")\n",
    "        print(\"  pth: \", pth)\n",
    "        print(\"  Input IDs: \", ids_row.tolist())\n",
    "        print(\"  Decoded:   \", decode_token_ids(ids_row.tolist()))\n",
    "        print(\"  Label IDs: \", lbl_row.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ac344",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in ds.take(1):\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    labels    = batch[\"labels\"]\n",
    "    loss_mask = batch[\"loss_mask\"]\n",
    "    \n",
    "j=19\n",
    "print(loss_mask[j])\n",
    "print('\\n',decode_token_ids(batch[\"input_ids\"][j].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472fb046",
   "metadata": {},
   "source": [
    "# Llama Architecture !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf08e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x   : (B, h, T, d) even-sized last dim (d must be multiple of 2)\n",
    "    sin : (T, d//2)     broadcastable\n",
    "    cos : (T, d//2)\n",
    "    \"\"\"\n",
    "    #This separates each feature vector's dimensions into 2 halves — like real and imaginary parts.\n",
    "    x_even = x[..., 0::2]      # Get even-dimension values → shape: (B, h, T, d/2)\n",
    "    x_odd  = x[..., 1::2]      # Get odd-dimension values → shape: (B, h, T, d/2)\n",
    "\n",
    "    # This is a 2D rotation formula applied to each positional index and head.\n",
    "    # It \"rotates\" the embedding vector in its dimensional space based on position.\n",
    "    x_rot_even =  x_even *  cos - x_odd * sin\n",
    "    x_rot_odd  =  x_even *  sin + x_odd * cos\n",
    "    \n",
    "    # interleave even/odd back together\n",
    "    x_rot = tf.stack([x_rot_even, x_rot_odd], axis=-1)   # (..., d/2, 2)\n",
    "    return tf.reshape(x_rot, tf.shape(x))                # (..., d)\n",
    "\n",
    "def make_sincos(seq_len, dim, base=10000):\n",
    "    '''\n",
    "    Returns sin, cos with shape (seq_len, dim//2)\n",
    "    '''\n",
    "    pos = tf.cast(tf.range(seq_len), tf.float32)                       # (T,)\n",
    "    i   = tf.cast(tf.range(0, dim, 2), tf.float32) / dim              # (d/2,)\n",
    "    theta = pos[:, None] / (base ** i[None, :])                       # (T, d/2)\n",
    "    return tf.sin(theta), tf.cos(theta)\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Vanilla multi-head (scaled-dot-product) attention implemented from scratch.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    d_model     : int   – total embedding size (must be divisible by num_heads)  \n",
    "    num_heads   : int   – number of attention heads  \n",
    "    dropout     : float – dropout on attention weights (0.0 = no dropout)\n",
    "\n",
    "    Call Signature\n",
    "    --------------\n",
    "    output, attn_scores = mha(\n",
    "        query,                     # (B, T_q, d_model)\n",
    "        value=None,                # (B, T_v, d_model)  – defaults to query\n",
    "        key=None,                  # (B, T_k, d_model)  – defaults to value\n",
    "        mask=None,                 # (B, 1, T_q, T_k) or (B, T_q, T_k)\n",
    "        use_causal_mask=False,     # True → autoregressive causal mask\n",
    "        training=None\n",
    "    )\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if d_model % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"d_model={d_model} must be divisible by num_heads={num_heads}\"\n",
    "            )\n",
    "\n",
    "        self.d_model   = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth     = d_model // num_heads\n",
    "\n",
    "        # Linear projections for Q, K, V and final output\n",
    "        self.wq   = tf.keras.layers.Dense(d_model, use_bias=False)\n",
    "        self.wk   = tf.keras.layers.Dense(d_model, use_bias=False)\n",
    "        self.wv   = tf.keras.layers.Dense(d_model, use_bias=False)\n",
    "        self.wo   = tf.keras.layers.Dense(d_model, use_bias=False)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────────\n",
    "    # Helpers\n",
    "    # ────────────────────────────────────────────────────────────────────────\n",
    "    def _split_heads(self, x, B):\n",
    "        \"\"\"\n",
    "        Reshape (B, T, d_model) → (B, num_heads, T, depth)\n",
    "        so we can run attention on each head in parallel.\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (B, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    @staticmethod\n",
    "    def _scaled_dot_product_attention(q, k, v, mask, dropout,training=None):\n",
    "        \"\"\"\n",
    "        Core attention: softmax(QKᵀ / √d_k) V\n",
    "        Returns: (B, h, T_q, depth_v), (B, h, T_q, T_k)\n",
    "        \"\"\"\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scores = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(dk)  # (B,h,T_q,T_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            # broadcast automatically if mask rank < scores rank\n",
    "            scores += (mask * -1e9)  # large negative → zero probability\n",
    "\n",
    "        attn = tf.nn.softmax(scores, axis=-1)\n",
    "        attn = dropout(attn,training=training)\n",
    "        output = tf.matmul(attn, v)  # (B,h,T_q,depth)\n",
    "        return output\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────────\n",
    "    # Forward pass\n",
    "    # ────────────────────────────────────────────────────────────────────────\n",
    "    def call(\n",
    "        self,\n",
    "        query,\n",
    "        value=None,\n",
    "        key=None,\n",
    "        mask=None,\n",
    "        use_causal_mask=False,\n",
    "        training=None\n",
    "    ):\n",
    "        if value is None:\n",
    "            value = query\n",
    "        if key is None:\n",
    "            key = value\n",
    "\n",
    "        B = tf.shape(query)[0]\n",
    "        Tq = tf.shape(query)[1]          # sequence length of Q\n",
    "        Tk = tf.shape(key)[1]\n",
    "\n",
    "        # 1. Linear projections\n",
    "        q = self.wq(query)   # (B, T_q, d_model)\n",
    "        k = self.wk(key)     # (B, T_k, d_model)\n",
    "        v = self.wv(value)   # (B, T_v, d_model)\n",
    "\n",
    "        # 2. Reshape for multi-head\n",
    "        q = self._split_heads(q, B)  # (B, h, T_q, depth)\n",
    "        k = self._split_heads(k, B)  # (B, h, T_k, depth)\n",
    "        v = self._split_heads(v, B)  # (B, h, T_v, depth)\n",
    "\n",
    "        # 3) -----------------  ROTARY  -----------------\n",
    "        # Build sin/cos for the longest sequence we need this step\n",
    "        max_len = tf.maximum(Tq, Tk)\n",
    "        sin, cos = make_sincos(max_len, self.depth)       # depth = d_model / num_heads\n",
    "\n",
    "        # Slice sin/cos to actual lengths (broadcast works automatically)\n",
    "        # RoPE modifies Q and K such that their dot product reflects not just content similarity but also relative position.\n",
    "        q = apply_rope(q, sin[:Tq], cos[:Tq])             # rotate Q\n",
    "        k = apply_rope(k, sin[:Tk], cos[:Tk])             # rotate K\n",
    "        # ----------------------------------------------\n",
    "\n",
    "        # 3. (Optional) Causal mask: block future positions\n",
    "        if use_causal_mask:\n",
    "            T_q = tf.shape(q)[2]\n",
    "            T_k = tf.shape(k)[2]\n",
    "            causal = 1.0 - tf.linalg.band_part(tf.ones((T_q, T_k)), -1, 0)  # lower-tri  # 1 → masked\n",
    "            causal = causal[tf.newaxis, tf.newaxis, :, :]  # (1,1,T_q,T_k)\n",
    "            mask = causal if mask is None else tf.maximum(mask, causal)\n",
    "\n",
    "        # 4. Scaled dot-product attention\n",
    "        attn_out = self._scaled_dot_product_attention(\n",
    "            q, k, v, mask, self.dropout,training=training,\n",
    "        )  # (B,h,T_q,depth), (B,h,T_q,T_k)\n",
    "\n",
    "        # 5. Concatenate heads\n",
    "        attn_out = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (B,T_q,h,depth)\n",
    "        attn_out = tf.reshape(attn_out, (B, -1, self.d_model))  # (B,T_q,d_model)\n",
    "\n",
    "        # 6. Final linear layer\n",
    "        output = self.wo(attn_out)  # (B,T_q,d_model)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class RMSNorm(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_size, epsilon=1e-8, **kwargs):\n",
    "        super(RMSNorm, self).__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Learnable scale parameter γ (same shape as last dim of input)\n",
    "        self.scale = self.add_weight(\n",
    "            name=\"scale\",\n",
    "            shape=(self.hidden_size,),\n",
    "            initializer=\"ones\",\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        rms = tf.sqrt(tf.reduce_mean(tf.square(x), axis=-1, keepdims=True) + self.epsilon)\n",
    "        norm_x = x / rms\n",
    "        return norm_x * self.scale\n",
    "\n",
    "class CausalSelfAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model=d_model,\n",
    "                                      num_heads=num_heads,\n",
    "                                      dropout=dropout)\n",
    "        self.rmsnorm = RMSNorm(d_model)\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, x, padding_mask=None, training=None):\n",
    "\n",
    "        rms_x1 = self.rmsnorm(x)\n",
    "\n",
    "        attn_output = self.mha(\n",
    "            query=rms_x1, value=rms_x1, key=rms_x1,\n",
    "            mask=padding_mask,          # may be None\n",
    "            use_causal_mask=True,\n",
    "            training=training,\n",
    "        )\n",
    "        rms_x1 = self.add([x, attn_output])\n",
    "        return rms_x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc25084",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim,factor=4):\n",
    "        super().__init__()\n",
    "        self.lin1 = tf.keras.layers.Dense(factor*hidden_dim,use_bias=False)   # W1\n",
    "        self.lin2 = tf.keras.layers.Dense(hidden_dim,use_bias=False)       # W2\n",
    "\n",
    "    def call(self, x):\n",
    "        x_ = self.lin1(x)                          # shape: (..., 4d)\n",
    "        a, b = tf.split(x_, num_or_size_splits=2, axis=-1)  # split\n",
    "        gated = a * (b * tf.sigmoid(b))            # SwiGLU: a ⊙ SiLU(b)\n",
    "        return self.lin2(gated)\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq = tf.keras.Sequential(\n",
    "            [\n",
    "                SwiGLU(d_model),\n",
    "                tf.keras.layers.Dropout(dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "        self.rmsnorm = RMSNorm(d_model)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        y = self.seq(self.rmsnorm(x), training=training)  # pre-norm\n",
    "        return x + y                                  # residual on raw x\n",
    "    \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.causal_self_attention = CausalSelfAttention(num_heads=num_heads, d_model=d_model, dropout=dropout_rate)\n",
    "        self.ffn = FeedForward(d_model)\n",
    "\n",
    "    def call(self, x, padding_mask=None, training=None):\n",
    "        x = self.causal_self_attention(x, padding_mask=padding_mask, training=training)\n",
    "        x = self.ffn(x,training=training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0214ddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self, *, num_layers, d_model, num_heads, dropout_rate=0.1,pad_token_id = pad_id):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "        self.token_embedding = token_model\n",
    "        self.img_embedding = build_image_encoder()\n",
    "\n",
    "        self.img_projection = tf.keras.layers.Dense(d_model)\n",
    "        # self.text_projection = tf.keras.layers.Dense(d_model) # not applicable if d_model != 128\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(d_model=d_model, num_heads=num_heads, dropout_rate=dropout_rate)\n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        pad_mask = tf.cast(tf.equal(inputs[1], self.pad_token_id), tf.float32)[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "        img = self.img_embedding(inputs[0],training=False)\n",
    "        img = self.img_projection(img)\n",
    "\n",
    "        text = self.token_embedding(inputs[1])\n",
    "        # text = self.text_projection(text)\n",
    "\n",
    "        # Make sure img_emb has the same dtype as text_emb\n",
    "        img = tf.cast(img, dtype=text.dtype)\n",
    "\n",
    "        # Replace text[:, 0, :] with img_emb\n",
    "        x = tf.concat([img, text[:, v3_output.shape[1]:, :]], axis=1)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for layer in self.dec_layers:\n",
    "            x = layer(x, padding_mask=pad_mask, training=training)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, input_vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            num_layers=num_layers,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "\n",
    "        self.rmsnorm = RMSNorm(d_model)\n",
    "        self.final_layer = tf.keras.layers.Dense(input_vocab_size)\n",
    "\n",
    "    def call(self, inputs,training=False):\n",
    "        # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "        # first argument.\n",
    "        x = inputs\n",
    "\n",
    "        x = self.decoder(x,training=training)  # (batch_size, target_len, d_model)\n",
    "\n",
    "        # Final linear layer output.\n",
    "        x = self.rmsnorm(x)\n",
    "        logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "        try:\n",
    "            # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "            # b/250038731\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        # Return the final output and the attention weights.\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be11746",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "d_model = 128   # if d_model != 128 then text_linear_projection\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "EPOCHS = 3\n",
    "\n",
    "model = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    input_vocab_size=VOCAB_SIZE,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "dummy_path   = tf.constant([\"vqa/rename/210032.jpg\"])   # (1,)  string\n",
    "images = tf.map_fn(decode_and_resize, dummy_path, fn_output_signature=tf.float32)\n",
    "dummy_tokens = tf.zeros((1, MAX_LEN), dtype=tf.int32)\n",
    "_ = model((images, dummy_tokens))              \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95ca277",
   "metadata": {},
   "outputs": [],
   "source": [
    "WARM_FRAC   = 0.1        # keep the fast ramp-up\n",
    "DECAY_RATE  = 4\n",
    "LR_FLOOR    = 1e-6\n",
    "LR_PEAK_DESIRED = 8e-4     # choose 8e-4 or 9e-4\n",
    "\n",
    "# pre-compute the scale that gives that peak\n",
    "num_steps     = EPOCHS * len(text_pairs) // BATCH_SIZE\n",
    "warmup_steps  = int(num_steps * WARM_FRAC)\n",
    "current_peak  = 1.0 / tf.sqrt(tf.cast(d_model * warmup_steps, tf.float32))\n",
    "LR_SCALE      = LR_PEAK_DESIRED / current_peak.numpy()\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()   # so it can round-trip in SavedModel/H5\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, total_steps=num_steps,\n",
    "                 warmup_frac=WARM_FRAC, decay_rate=DECAY_RATE,\n",
    "                 lr_scale=LR_SCALE):\n",
    "        super().__init__()\n",
    "        self.d_model      = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = tf.cast(int(total_steps * warmup_frac), tf.float32)\n",
    "        self.decay_rate   = decay_rate\n",
    "        self.decay_steps  = tf.cast(total_steps, tf.float32)\n",
    "        self.lr_scale     = tf.cast(lr_scale, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step  = tf.cast(step, tf.float32)\n",
    "        arg1  = tf.math.rsqrt(step)\n",
    "        arg2  = step * tf.math.pow(self.warmup_steps, -1.5)\n",
    "        warm  = tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "        decay = tf.math.exp(-self.decay_rate *\n",
    "                            tf.maximum(step - self.warmup_steps, 0.) /\n",
    "                            self.decay_steps)\n",
    "        lr = warm * decay * self.lr_scale \n",
    "        return tf.maximum(lr, LR_FLOOR)\n",
    "\n",
    "    # ----------  NEW  ----------\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"d_model\":      int(self.d_model.numpy()),   # cast back to Python types\n",
    "            \"total_steps\":  int(self.decay_steps.numpy()),\n",
    "            \"warmup_frac\":  float(self.warmup_steps.numpy() / self.decay_steps.numpy()),\n",
    "            \"decay_rate\":   self.decay_rate,\n",
    "            \"lr_scale\":     float(self.lr_scale.numpy()),\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    \n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9, clipnorm=1.0)\n",
    "\n",
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(num_steps, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")\n",
    "\n",
    "print(temp_learning_rate_schedule(tf.range(num_steps, dtype=tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0db3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_for_fit = ds.map(\n",
    "    lambda b: (\n",
    "        (tf.map_fn(decode_and_resize, b['img_path'], fn_output_signature=tf.float32), b[\"input_ids\"]),\n",
    "        b[\"labels\"],\n",
    "        b[\"loss_mask\"]\n",
    "    ),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "del(ds)\n",
    "\n",
    "# 3) Compile with a standard sparse‐CE loss and let Keras use sample weights\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\"),\n",
    "    metrics=[\"sparse_categorical_accuracy\"]\n",
    ")\n",
    "\n",
    "# Build callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"loss\", patience=3,\n",
    "                  restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint(\n",
    "        filepath=\"best_summary.keras\",        # or \"best_summary.h5\"\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=True,\n",
    "        verbose=1             # full model (weights + optimizer + LR schedule)\n",
    "    )\n",
    "]\n",
    "\n",
    "# 4) Fit!  Keras will print epoch/step progress by default\n",
    "history = model.fit(\n",
    "    ds_for_fit,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1    # 1 = progress bar, loss & acc per epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc00feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str, max_len: int = MAX_LEN) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Returns [BOS] dialogue [EOS] *without* any padding.\n",
    "    \"\"\"\n",
    "    ids = custom_tokenize(text)               # np padding\n",
    "    return tf.constant(ids, tf.int32)              # 1-D tensor\n",
    "\n",
    "\n",
    "# ── 2.  Greedy decoding loop  ─────────────────────────────────────────\n",
    "def generate_answer(image_path: str,\n",
    "                    question:   str,\n",
    "                    max_new_tokens: int = 30) -> str:\n",
    "    \n",
    "    img_token = tf.constant([image_path])\n",
    "    img_token = tf.map_fn(decode_and_resize, img_token, fn_output_signature=tf.float32)\n",
    "    \n",
    "    text_tokens = encode(question)\n",
    "    text_tokens = text_tokens[tf.newaxis, :]   \n",
    "\n",
    "    # 2-C.  Autoregressive loop\n",
    "    for _ in range(max_new_tokens):\n",
    "        # logits : (1, cur_len, vocab)\n",
    "        logits = model.predict((img_token, text_tokens),verbose=False)[:, -1, :] / 0.5        # last position\n",
    "        next_id = tf.random.categorical(logits, 1, dtype=tf.int32)\n",
    "\n",
    "        # append\n",
    "        text_tokens = tf.concat([text_tokens, next_id], 1)\n",
    "\n",
    "        # stop on EOS\n",
    "        if int(next_id[0]) == eos_id or text_tokens.shape[1] >= MAX_LEN:\n",
    "            break\n",
    "\n",
    "    generated = text_tokens.numpy().tolist()[0] \n",
    "\n",
    "    imgbgr = cv2.imread(image_path)\n",
    "    imgrgb = cv2.cvtColor(imgbgr,cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(imgrgb)\n",
    "    plt.show()\n",
    "\n",
    "    return tokenizer.decode(generated)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b0056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img   = \"vqa/rename/13291.jpg\"\n",
    "ques  = \"what are they getting ready to do?\"\n",
    "print(generate_answer(img, ques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0779c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_pool = [\n",
    "    \"What is present in the image?\",\n",
    "    \"How would you describe the background setting?\",\n",
    "    \"What is the central subject or point of focus?\",\n",
    "    \"What is happening in the image?\",\n",
    "    \"Where might this photo have been taken?\",\n",
    "    \"Describe the image.\",\n",
    "    \"Are there people in the picture?\",\n",
    "    \"Is there an animal in the picture?\",\n",
    "    \"What objects are visible in the image?\",\n",
    "    \"What activity or event is occurring in the image?\",\n",
    "    \"Can you describe the colors present in the image?\",\n",
    "    \"What do you think the mood or atmosphere of the image is?\",\n",
    "    \"What draws your attention most in the image?\",\n",
    "    \"Does the image appear to be candid or posed?\",\n",
    "    \"Is there any text or signage visible in the image?\",\n",
    "    \"Are there any buildings or structures visible?\",\n",
    "    \"Does the image convey any emotions or feelings?\",\n",
    "    \"Are there any artistic or stylistic elements in the photo?\",\n",
    "    \"If you could give this image a title, what would it be?\",\n",
    "    'What is unique about the image?',\n",
    "    'What can you tell?',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fbc0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img   = \"vqa/rename/74.jpg\"\n",
    "ques  = \"where is the dog ?\"\n",
    "print(generate_answer(img, ques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b33f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "img   = \"vqa/rename/7519.jpg\"\n",
    "ques  = \"what is the color of the shirt?\"\n",
    "print(generate_answer(img, ques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2e14c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26609ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(20):\n",
    "    print('\\n',j)\n",
    "    img   = df2['img_path'][j]\n",
    "    ques  = df2['input_text'][j]\n",
    "    print(generate_answer(img, ques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e304cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Extraas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
