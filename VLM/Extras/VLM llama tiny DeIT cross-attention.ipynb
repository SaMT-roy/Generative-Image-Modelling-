{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae68aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50998372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read the JSON file  https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain\n",
    "with open(\"blip_laion_cc_sbu_558k.json\", \"r\") as file:\n",
    "    data = json.load(file)  # Parse the JSON data into a Python dictionary\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame([{\n",
    "    'id': item['id'],\n",
    "    'img_path': 'images/'+item['image'],\n",
    "    'input_text': next(conv['value'] for conv in item['conversations'] if conv['from'] == 'human'),\n",
    "    'target_text': next(conv['value'] for conv in item['conversations'] if conv['from'] == 'gpt')\n",
    "} for item in data]).dropna().reset_index(drop=True)\n",
    "\n",
    "del(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Text cleaning ------------------------------------------------------\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove HTML, normalize whitespace, preserve punctuation/numbers/casing.\n",
    "    \"\"\"\n",
    "    # text = text.lower()\n",
    "    text = re.sub(r'<image>', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'[^a-zA-Z0-9.,!?]+', ' ', text)\n",
    "    text = text.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "df['target_text'] = df['target_text'].apply(lambda x: clean_text(x))\n",
    "df['input_text'] = df['input_text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93908444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine summary + dialogue and split on whitespace\n",
    "raw_lens = [\n",
    "    len(f\"{s} {d}\".split()) \n",
    "    for s, d in tqdm(zip(df['target_text'], df['input_text']), total=len(df))\n",
    "]\n",
    "\n",
    "lens = np.array(raw_lens)\n",
    "\n",
    "# Print summary stats\n",
    "def pct(x): return np.percentile(lens, x)\n",
    "\n",
    "print(f\"Total examples    : {len(lens):,}\")\n",
    "print(f\"Min / Max words   : {lens.min()} / {lens.max()}\")\n",
    "print(f\"Mean Â± std        : {lens.mean():.1f} Â± {lens.std():.1f}\")\n",
    "print(\"--- Percentiles (word count per raw text pair) ---\")\n",
    "for p in [50, 90, 95, 98, 99]:\n",
    "    print(f\"{p:>3}% : {pct(p):.0f} words\")\n",
    "\n",
    "del(raw_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9306851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = int(pct(99))\n",
    "\n",
    "text_pairs = []\n",
    "\n",
    "for img,i,j in zip(df.img_path,df.input_text,df.target_text):\n",
    "    try:\n",
    "        if len(i.split(\" \")+j.split(\" \")) < MAX_LEN: \n",
    "            text_pairs.append((img,i,j))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "len(text_pairs), text_pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a83da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFViTModel, ViTImageProcessor\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained(\"facebook/deit-tiny-patch16-224\")\n",
    "IMAGE_MEAN = tf.constant(processor.image_mean) # e.g., [0.485, 0.456, 0.406]\n",
    "IMAGE_STD = tf.constant(processor.image_std)   # e.g., [0.229, 0.224, 0.225]\n",
    "TARGET_SIZE = (processor.size[\"height\"], processor.size[\"width\"]) # e.g., (224, 224)\n",
    "\n",
    "IMAGE_MEAN,IMAGE_STD,TARGET_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c984539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_and_resize(image_path):\n",
    "    \"\"\"\n",
    "    A pure TensorFlow function for loading and preprocessing an image for ViT.\n",
    "    This function can be efficiently parallelized in a tf.data pipeline.\n",
    "    \"\"\"\n",
    "    # 1. Read and Decode Image (Fast TF I/O)\n",
    "    img_bytes = tf.io.read_file(image_path)\n",
    "    # Use decode_image to handle both JPEG and PNG robustly\n",
    "    img = tf.image.decode_image(img_bytes, channels=3, expand_animations=False)\n",
    "    \n",
    "    # 2. Resize Image (Fast TF op)\n",
    "    # The 'bicubic' method is standard for ViT models.\n",
    "    img = tf.image.resize(img, TARGET_SIZE, method='bicubic')\n",
    "    \n",
    "    # 3. Convert dtype and rescale to [0, 1] (Fast TF op)\n",
    "    # This replaces the internal scaling of the processor.\n",
    "    img = tf.image.convert_image_dtype(img, dtype=tf.float32)\n",
    "    \n",
    "    # 4. Normalize using the model's specific mean and std (Fast TF op)\n",
    "    img = (img - IMAGE_MEAN) / IMAGE_STD\n",
    "    \n",
    "    # 5. Transpose channels for the model (Fast TF op)\n",
    "    # ViT models expect (channels, height, width), but TF loads as (height, width, channels).\n",
    "    img = tf.transpose(img, perm=[2, 0, 1])\n",
    "    \n",
    "    return img\n",
    "\n",
    "class ViTFeatureExtractor(tf.keras.layers.Layer):\n",
    "    def __init__(self, model_name=\"facebook/deit-tiny-patch16-224\"):\n",
    "        super().__init__()\n",
    "        self.vit = TFViTModel.from_pretrained(model_name)\n",
    "        self.vit.trainable = False\n",
    "\n",
    "    def call(self, pixel_values):\n",
    "        outputs = self.vit(pixel_values)\n",
    "        \n",
    "        # Remove the [CLS] token\n",
    "        patch_tokens = outputs.last_hidden_state[:, 1:, :]\n",
    "        return patch_tokens                              \n",
    "\n",
    "paths = tf.constant(['/Users/saptarshimallikthakur/Desktop/d39c160d9ed1a406519e4915c4b43e05.jpg','/Users/saptarshimallikthakur/Desktop/510afe0f8b983dfbd7b6c066b94135d2.jpg'])\n",
    "images = tf.map_fn(decode_and_resize, paths, fn_output_signature=tf.float32)\n",
    "model = ViTFeatureExtractor()\n",
    "v3_output = model(images)\n",
    "v3_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5121e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers import models, trainers, pre_tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8301a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ assume train_df already loaded â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "rng            = np.random.default_rng(42)\n",
    "valid_mask     = rng.random(len(df)) < 0.25          # 25 % hold-out\n",
    "train_df_, valid_df = df[~valid_mask], df[valid_mask]\n",
    "\n",
    "# â‡¢ cache validation texts so we can loop over them many times\n",
    "valid_texts = [f\"{s} {d}\" for s, d in zip(valid_df[\"input_text\"], valid_df[\"target_text\"])]\n",
    "\n",
    "def train_tokenizer(text_iter, vocab_size: int) -> Tokenizer:\n",
    "    tok = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "    tok.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size,\n",
    "                                  min_frequency=2,\n",
    "                                  special_tokens=[\"[UNK]\"])\n",
    "    tok.train_from_iterator(text_iter, trainer)\n",
    "    return tok\n",
    "\n",
    "def avg_pieces_per_word(tok: Tokenizer, texts) -> float:\n",
    "    pieces = words = 0\n",
    "    for t in texts:\n",
    "        ids   = tok.encode(t).ids\n",
    "        pieces += len(ids)\n",
    "        words  += len(t.split())\n",
    "    return pieces / words\n",
    "\n",
    "vocab_sizes = [1000,2000,4000,6000,8000,10000,12000]\n",
    "pieces_per_word, oov_rate = [], []\n",
    "\n",
    "for k in vocab_sizes:\n",
    "    print(k)\n",
    "    train_iter = (f\"{s} {d}\" for s, d in\n",
    "                  zip(train_df_[\"input_text\"], train_df_[\"target_text\"]))\n",
    "    tok = train_tokenizer(train_iter, k)\n",
    "\n",
    "    # 1) average sub-words per word on validation set\n",
    "    pieces_per_word.append(avg_pieces_per_word(tok, valid_texts))\n",
    "\n",
    "    # 2) OOV percentage on validation set\n",
    "    unk_id = tok.token_to_id(\"[UNK]\")\n",
    "    total = unk = 0\n",
    "    for t in valid_texts:\n",
    "        ids   = tok.encode(t).ids\n",
    "        total += len(ids)\n",
    "        unk   += sum(id_ == unk_id for id_ in ids)\n",
    "    oov_rate.append(100 * unk / total)\n",
    "\n",
    "# â”€â”€ plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "plt.figure(figsize=(7,3))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(vocab_sizes, pieces_per_word, marker='o')\n",
    "plt.title(\"pieces / word\"); plt.xlabel(\"vocab size\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(vocab_sizes, oov_rate, marker='o')\n",
    "plt.title(\"OOV %\"); plt.xlabel(\"vocab size\")\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a960e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 8000\n",
    "\n",
    "CORPUS_FILE = 'all_texts.txt'\n",
    "SPECIAL_TOKENS = ['[PAD]','[UNK]','[BOS]','[EOS]','[SEP]'] \n",
    "\n",
    "with open(CORPUS_FILE, 'w', encoding='utf-8') as f:\n",
    "    for _, row in df.iterrows():\n",
    "        f.write(str(row['input_text']) + ' ' + str(row['target_text']) + '\\n')\n",
    "\n",
    "# Initialize and train\n",
    "tokenizer = Tokenizer(BPE(unk_token='[UNK]'))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=SPECIAL_TOKENS)\n",
    "tokenizer.train([CORPUS_FILE], trainer)\n",
    "\n",
    "# 3) Configure post-processing, padding & truncation ------------------\n",
    "pad_id = tokenizer.token_to_id('[PAD]')\n",
    "unk_id = tokenizer.token_to_id('[UNK]')\n",
    "bos_id = tokenizer.token_to_id('[BOS]')\n",
    "eos_id = tokenizer.token_to_id('[EOS]')\n",
    "sep_id = tokenizer.token_to_id('[SEP]')\n",
    "\n",
    "# Add BOS/EOS around single sequences\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single='[BOS] $A [SEP]',\n",
    "    pair='[BOS] $A [SEP] $B [EOS]',\n",
    "    special_tokens=[('[BOS]', bos_id), ('[EOS]', eos_id),('[SEP]', sep_id)]\n",
    ")\n",
    "\n",
    "# 0)  Make sure `tok` points to the FINAL tokenizer\n",
    "#     â†’ trained on the full corpus with your chosen vocab_size\n",
    "# ------------------------------------------------------------\n",
    "tok = tokenizer          # whatever variable you use\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1)  Get token counts for every example\n",
    "#     (here: concatenate summary + dialogue; split if you want)\n",
    "# ------------------------------------------------------------\n",
    "lens = []\n",
    "\n",
    "for s, d in tqdm(zip(df[\"input_text\"], df[\"target_text\"]),total=len(df)):\n",
    "    txt   = f\"{s} {d}\"\n",
    "    ids   = tok.encode(txt).ids\n",
    "    lens.append(len(ids))\n",
    "\n",
    "lens = np.array(lens)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2)  Print key stats\n",
    "# ------------------------------------------------------------\n",
    "def pct(x): return np.percentile(lens, x)\n",
    "\n",
    "print(f\"Total samples    : {len(lens):,}\")\n",
    "print(f\"Min / Max tokens : {lens.min()} / {lens.max()}\")\n",
    "print(f\"Mean Â± std       : {lens.mean():.1f} Â± {lens.std():.1f}\")\n",
    "print(\"--- percentiles (tokens) ---\")\n",
    "for p in (50, 90, 95, 98, 99):\n",
    "    print(f\"{p:>3}% : {pct(p):.0f}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3)  Quick histogram    (no seaborn, single plot, no colors set)\n",
    "# ------------------------------------------------------------\n",
    "plt.hist(lens, bins=50)\n",
    "plt.xlabel(\"tokens per example\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Tokenised length distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f670b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_LEN = int(pct(99))\n",
    "del(lens)\n",
    "\n",
    "# 0) Configure your tokenizer *once* at startup, not per-example\n",
    "tokenizer.enable_truncation(\n",
    "    max_length=MAX_LEN\n",
    ")\n",
    "\n",
    "tokenizer.enable_padding(\n",
    "    pad_id=pad_id,\n",
    "    pad_token='[PAD]',\n",
    "    length=MAX_LEN\n",
    ")\n",
    "\n",
    "def encode_text(texts):\n",
    "    encodings = tokenizer.encode(texts).ids\n",
    "    return np.array(encodings, dtype=np.int32)\n",
    "\n",
    "# 1) encode_pair â†’ return a NumPy array\n",
    "def encode_pair(text_a: str, text_b: str) -> np.ndarray:\n",
    "    enc = tokenizer.encode(text_a, text_b).ids\n",
    "    return np.array(enc, dtype=np.int32)   # shape: [MAX_LEN]\n",
    "\n",
    "def encode_example(img_path, text: str, summary: str):\n",
    "    ids = encode_pair(text, summary)           # np.ndarray, shape=[MAX_LEN]\n",
    "    labels = np.concatenate([ids[1:], [pad_id]])  # shape=[MAX_LEN]\n",
    "\n",
    "    # find SEP\n",
    "    sep_idxs = np.where(labels == sep_id)[0]\n",
    "    sep_pos = int(sep_idxs[0]) if sep_idxs.size else len(ids)\n",
    "\n",
    "    # build base mask: 1 only for positions > sep_pos AND not PAD\n",
    "    positions = np.arange(len(labels))\n",
    "    loss_mask = (positions > sep_pos).astype(np.float32) * (labels != pad_id).astype(np.float32)\n",
    "\n",
    "    return img_path, ids, labels.astype(np.int32), loss_mask\n",
    "\n",
    "# â”€â”€ 0. do *all* tokenisation once â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "triples = [encode_example(img, t, s) for (img, t, s) in text_pairs]   # Python loop, done **once**\n",
    "img_path, ids, labels, masks = map(lambda k: tf.constant(np.stack(k, 0)),zip(*triples))                      # shapes [N, MAX_LEN]\n",
    "\n",
    "# â”€â”€ 1. build the purely-TF dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ds = (\n",
    "    tf.data.Dataset.from_tensor_slices({'img_path':img_path,\"input_ids\": ids, \"labels\": labels, \"loss_mask\": masks})\n",
    "    .shuffle(len(text_pairs))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "del(triples)\n",
    "del(img_path)\n",
    "del(ids)\n",
    "del(labels)\n",
    "del(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706734f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()           # indices 0 â€¦ V\n",
    "\n",
    "id_to_token = {idx: tok for tok, idx in vocab.items()}\n",
    "\n",
    "# 2) Decoder: drop PADs\n",
    "def decode_token_ids(token_ids: list[int]) -> str:\n",
    "    tokens = []\n",
    "    for tid in token_ids:\n",
    "        if tid == pad_id:\n",
    "            continue\n",
    "        tok = id_to_token.get(tid, '?')\n",
    "        if tok == '$':\n",
    "            continue  # Skip the '$' symbol\n",
    "        if tok.startswith('Ä '):\n",
    "            tok = tok[1:]  # Remove the space prefix indicator\n",
    "            tokens.append(' ' + tok)\n",
    "        else:\n",
    "            tokens.append(tok)\n",
    "    return ' '.join(tokens).strip()\n",
    "\n",
    "# 3) Inspect one batch from your TF dataset\"\n",
    "for batch in ds.take(10):\n",
    "    pth = batch['img_path'].numpy()\n",
    "    input_ids = batch['input_ids'].numpy()  # shape (batch, MAX_LEN)\n",
    "    labels    = batch['labels'].numpy()\n",
    "\n",
    "    for i, (pth, ids_row, lbl_row) in enumerate(zip(pth, input_ids, labels), start=1):\n",
    "        print(f\"\\nðŸŸ¢ Sample {i}\")\n",
    "        print(\"  pth: \", pth)\n",
    "        print(\"  Input IDs: \", ids_row.tolist())\n",
    "        print(\"  Decoded:   \", decode_token_ids(ids_row.tolist()))\n",
    "        print(\"  Label IDs: \", lbl_row.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422d8a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in ds.take(1):\n",
    "    break\n",
    "    \n",
    "j=19\n",
    "print(batch[\"loss_mask\"][j])\n",
    "print('\\n',decode_token_ids(batch[\"labels\"][j].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da6208e",
   "metadata": {},
   "source": [
    "# LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4452daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x   : (B, h, T, d) even-sized last dim (d must be multiple of 2)\n",
    "    sin : (T, d//2)     broadcastable\n",
    "    cos : (T, d//2)\n",
    "    \"\"\"\n",
    "    #This separates each feature vector's dimensions into 2 halves â€” like real and imaginary parts.\n",
    "    x_even = x[..., 0::2]      # Get even-dimension values â†’ shape: (B, h, T, d/2)\n",
    "    x_odd  = x[..., 1::2]      # Get odd-dimension values â†’ shape: (B, h, T, d/2)\n",
    "\n",
    "    # This is a 2D rotation formula applied to each positional index and head.\n",
    "    # It \"rotates\" the embedding vector in its dimensional space based on position.\n",
    "    x_rot_even =  x_even *  cos - x_odd * sin\n",
    "    x_rot_odd  =  x_even *  sin + x_odd * cos\n",
    "    \n",
    "    # interleave even/odd back together\n",
    "    x_rot = tf.stack([x_rot_even, x_rot_odd], axis=-1)   # (..., d/2, 2)\n",
    "    return tf.reshape(x_rot, tf.shape(x))                # (..., d)\n",
    "\n",
    "def make_sincos(seq_len, dim, base=10000):\n",
    "    '''\n",
    "    Returns sin, cos with shape (seq_len, dim//2)\n",
    "    '''\n",
    "    pos = tf.cast(tf.range(seq_len), tf.float32)                       # (T,)\n",
    "    i   = tf.cast(tf.range(0, dim, 2), tf.float32) / dim              # (d/2,)\n",
    "    theta = pos[:, None] / (base ** i[None, :])                       # (T, d/2)\n",
    "    return tf.sin(theta), tf.cos(theta)\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Vanilla multi-head (scaled-dot-product) attention implemented from scratch.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    d_model     : int   â€“ total embedding size (must be divisible by num_heads)  \n",
    "    num_heads   : int   â€“ number of attention heads  \n",
    "    dropout     : float â€“ dropout on attention weights (0.0 = no dropout)\n",
    "\n",
    "    Call Signature\n",
    "    --------------\n",
    "    output, attn_scores = mha(\n",
    "        query,                     # (B, T_q, d_model)\n",
    "        value=None,                # (B, T_v, d_model)  â€“ defaults to query\n",
    "        key=None,                  # (B, T_k, d_model)  â€“ defaults to value\n",
    "        mask=None,                 # (B, 1, T_q, T_k) or (B, T_q, T_k)\n",
    "        use_causal_mask=False,     #  True â†’ autoregressive causal mask\n",
    "        training=None\n",
    "    )\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if d_model % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"d_model={d_model} must be divisible by num_heads={num_heads}\"\n",
    "            )\n",
    "\n",
    "        self.d_model   = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth     = d_model // num_heads\n",
    "\n",
    "        # Linear projections for Q, K, V and final output\n",
    "        self.wq   = tf.keras.layers.Dense(d_model, use_bias=False)\n",
    "        self.wk   = tf.keras.layers.Dense(d_model, use_bias=False)\n",
    "        self.wv   = tf.keras.layers.Dense(d_model, use_bias=False)\n",
    "        self.wo   = tf.keras.layers.Dense(d_model, use_bias=False)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Helpers\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    def _split_heads(self, x, B):\n",
    "        \"\"\"\n",
    "        Reshape (B, T, d_model) â†’ (B, num_heads, T, depth)\n",
    "        so we can run attention on each head in parallel.\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (B, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    @staticmethod\n",
    "    def _scaled_dot_product_attention(q, k, v, mask, dropout,training=None):\n",
    "        \"\"\"\n",
    "        Core attention: softmax(QKáµ€ / âˆšd_k) V\n",
    "        Returns: (B, h, T_q, depth_v), (B, h, T_q, T_k)\n",
    "        \"\"\"\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scores = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(dk)  # (B,h,T_q,T_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            # broadcast automatically if mask rank < scores rank\n",
    "            scores += (mask * -1e9)  # large negative â†’ zero probability\n",
    "\n",
    "        attn = tf.nn.softmax(scores, axis=-1)\n",
    "        attn = dropout(attn,training=training)\n",
    "        output = tf.matmul(attn, v)  # (B,h,T_q,depth)\n",
    "        return output\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Forward pass\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    def call(\n",
    "        self,\n",
    "        query,\n",
    "        value=None,\n",
    "        key=None,\n",
    "        mask=None,\n",
    "        use_causal_mask=False,\n",
    "        training=None,\n",
    "        self_attention=False\n",
    "    ):\n",
    "        if value is None:\n",
    "            value = query\n",
    "        if key is None:\n",
    "            key = value\n",
    "\n",
    "        B = tf.shape(query)[0]\n",
    "        Tq = tf.shape(query)[1]          # sequence length of Q\n",
    "        Tk = tf.shape(key)[1]\n",
    "\n",
    "        # 1. Linear projections\n",
    "        q = self.wq(query)   # (B, T_q, d_model)\n",
    "        k = self.wk(key)     # (B, T_k, d_model)\n",
    "        v = self.wv(value)   # (B, T_v, d_model)\n",
    "\n",
    "        # 2. Reshape for multi-head\n",
    "        q = self._split_heads(q, B)  # (B, h, T_q, depth)\n",
    "        k = self._split_heads(k, B)  # (B, h, T_k, depth)\n",
    "        v = self._split_heads(v, B)  # (B, h, T_v, depth)\n",
    "\n",
    "        # 3) -----------------  ROTARY  -----------------\n",
    "        # Build sin/cos for the longest sequence we need this step\n",
    "        max_len = tf.maximum(Tq, Tk)\n",
    "        sin, cos = make_sincos(max_len, self.depth)       # depth = d_model / num_heads\n",
    "\n",
    "        # Slice sin/cos to actual lengths (broadcast works automatically)\n",
    "        # RoPE modifies Q and K such that their dot product reflects not just content similarity but also relative position.\n",
    "        if self_attention:\n",
    "            q = apply_rope(q, sin[:Tq], cos[:Tq])             # rotate Q\n",
    "            k = apply_rope(k, sin[:Tk], cos[:Tk])             # rotate K\n",
    "        # ----------------------------------------------\n",
    "\n",
    "        # 3. (Optional) Causal mask: block future positions\n",
    "        if use_causal_mask:\n",
    "            T_q = tf.shape(q)[2]\n",
    "            T_k = tf.shape(k)[2]\n",
    "            causal = 1.0 - tf.linalg.band_part(tf.ones((T_q, T_k)), -1, 0)  # lower-tri  # 1 â†’ masked\n",
    "            causal = causal[tf.newaxis, tf.newaxis, :, :]  # (1,1,T_q,T_k)\n",
    "            mask = causal if mask is None else tf.maximum(mask, causal)\n",
    "\n",
    "        # 4. Scaled dot-product attention\n",
    "        attn_out = self._scaled_dot_product_attention(\n",
    "            q, k, v, mask, self.dropout,training=training,\n",
    "        )  # (B,h,T_q,depth), (B,h,T_q,T_k)\n",
    "\n",
    "        # 5. Concatenate heads\n",
    "        attn_out = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (B,T_q,h,depth)\n",
    "        attn_out = tf.reshape(attn_out, (B, -1, self.d_model))  # (B,T_q,d_model)\n",
    "\n",
    "        # 6. Final linear layer\n",
    "        output = self.wo(attn_out)  # (B,T_q,d_model)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class RMSNorm(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_size, epsilon=1e-8, **kwargs):\n",
    "        super(RMSNorm, self).__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Learnable scale parameter Î³ (same shape as last dim of input)\n",
    "        self.scale = self.add_weight(\n",
    "            name=\"scale\",\n",
    "            shape=(self.hidden_size,),\n",
    "            initializer=\"ones\",\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        rms = tf.sqrt(tf.reduce_mean(tf.square(x), axis=-1, keepdims=True) + self.epsilon)\n",
    "        norm_x = x / rms\n",
    "        return norm_x * self.scale\n",
    "\n",
    "class CausalSelfAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model=d_model,num_heads=num_heads,dropout=dropout)\n",
    "        self.rmsnorm = RMSNorm(d_model)\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, x, padding_mask=None, training=None):\n",
    "\n",
    "        rms_x1 = self.rmsnorm(x)\n",
    "        attn_output = self.mha(\n",
    "            query=rms_x1, value=rms_x1, key=rms_x1,\n",
    "            mask=padding_mask,          # may be None\n",
    "            use_causal_mask=True,\n",
    "            training=training,\n",
    "            self_attention=True\n",
    "        )\n",
    "        rms_x1 = self.add([x, attn_output])\n",
    "        return rms_x1\n",
    "    \n",
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Q = text, K=V = image patches (already projected).\"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.mha =  MultiHeadAttention(d_model=d_model,num_heads=num_heads,dropout=dropout)\n",
    "        self.rmsnorm = RMSNorm(d_model)\n",
    "    def call(self, x, kv, training=None):\n",
    "        norm_x = self.rmsnorm(x)\n",
    "        attn_out = self.mha(query=norm_x, key=kv, value=kv, training=training)\n",
    "        return x + attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc4ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim,factor=4):\n",
    "        super().__init__()\n",
    "        self.lin1 = tf.keras.layers.Dense(factor*hidden_dim,use_bias=False)   # W1\n",
    "        self.lin2 = tf.keras.layers.Dense(hidden_dim,use_bias=False)       # W2\n",
    "\n",
    "    def call(self, x):\n",
    "        x_ = self.lin1(x)                          # shape: (..., 4d)\n",
    "        a, b = tf.split(x_, num_or_size_splits=2, axis=-1)  # split\n",
    "        gated = a * (b * tf.sigmoid(b))            # SwiGLU: a âŠ™ SiLU(b)\n",
    "        return self.lin2(gated)\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq = tf.keras.Sequential(\n",
    "            [\n",
    "                SwiGLU(d_model),\n",
    "                tf.keras.layers.Dropout(dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "        self.rmsnorm = RMSNorm(d_model)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        y = self.seq(self.rmsnorm(x), training=training)  # pre-norm\n",
    "        return x + y                                  # residual on raw x\n",
    "    \n",
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.causal_self_attention = CausalSelfAttention(num_heads=num_heads, d_model=d_model, dropout=dropout_rate)\n",
    "        self.cross_attn = CrossAttention(num_heads=num_heads, d_model=d_model, dropout=dropout_rate)\n",
    "        self.ffn = FeedForward(d_model)\n",
    "\n",
    "    def call(self, x, kv, padding_mask=None, training=None):\n",
    "        x = self.causal_self_attention(x, padding_mask=padding_mask, training=training)\n",
    "        x = self.cross_attn(x, kv, training=training)\n",
    "        x = self.ffn(x,training=training)\n",
    "        return x\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self, *, num_layers, d_model, num_heads, dropout_rate=0.1,pad_token_id = pad_id):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "        self.token_embedding = tf.keras.layers.Embedding(input_dim=VOCAB_SIZE,output_dim=d_model)\n",
    "        self.img_embedding = ViTFeatureExtractor()\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.dec_layers = [DecoderBlock(d_model=d_model, num_heads=num_heads, dropout_rate=dropout_rate) for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        imgs, text_ids = inputs  # imgs:(B,H,W,3)  text_ids:(B,T)\n",
    "        pad_mask = tf.cast(tf.equal(text_ids, self.pad_token_id), tf.float32)[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "        kv = self.img_embedding(imgs,training=False)\n",
    "        x = self.token_embedding(text_ids)\n",
    "\n",
    "        # Make sure img_emb has the same dtype as text_emb\n",
    "        kv = tf.cast(kv, dtype=x.dtype)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for layer in self.dec_layers:\n",
    "            x = layer(x, kv, padding_mask=pad_mask, training=training)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, input_vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.decoder = Decoder(num_layers=num_layers,d_model=d_model,num_heads=num_heads,dropout_rate=dropout_rate,)\n",
    "        self.rmsnorm = RMSNorm(d_model)\n",
    "        self.final_layer = tf.keras.layers.Dense(input_vocab_size)\n",
    "\n",
    "    def call(self, inputs,training=False):\n",
    "        x = self.decoder(inputs,training=training)  # (batch_size, target_len, d_model)\n",
    "        x = self.rmsnorm(x)\n",
    "        logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac31250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "v3_output.shape[-1],v3_output.shape[-1]/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1332771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 6\n",
    "d_model = v3_output.shape[-1]\n",
    "num_heads = v3_output.shape[-1]//32\n",
    "dropout_rate = 0.1\n",
    "EPOCHS = 2\n",
    "\n",
    "model = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    input_vocab_size=VOCAB_SIZE,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "dummy_path   = tf.constant([\"/Users/saptarshimallikthakur/Desktop/d39c160d9ed1a406519e4915c4b43e05.jpg\"])   # (1,)  string\n",
    "images = tf.map_fn(decode_and_resize, dummy_path, fn_output_signature=tf.float32)\n",
    "dummy_tokens = tf.zeros((1, MAX_LEN), dtype=tf.int32)\n",
    "_ = model((images, dummy_tokens))              \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce93fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "WARM_FRAC   = 0.1        # keep the fast ramp-up\n",
    "DECAY_RATE  = 4\n",
    "LR_FLOOR    = 1e-6\n",
    "LR_PEAK_DESIRED = 8e-4     # choose 8e-4 or 9e-4\n",
    "\n",
    "# pre-compute the scale that gives that peak\n",
    "num_steps     = EPOCHS * len(text_pairs) // BATCH_SIZE\n",
    "warmup_steps  = int(num_steps * WARM_FRAC)\n",
    "current_peak  = 1.0 / tf.sqrt(tf.cast(d_model * warmup_steps, tf.float32))\n",
    "LR_SCALE      = LR_PEAK_DESIRED / current_peak.numpy()\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()   # so it can round-trip in SavedModel/H5\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, total_steps=num_steps,\n",
    "                 warmup_frac=WARM_FRAC, decay_rate=DECAY_RATE,\n",
    "                 lr_scale=LR_SCALE):\n",
    "        super().__init__()\n",
    "        self.d_model      = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = tf.cast(int(total_steps * warmup_frac), tf.float32)\n",
    "        self.decay_rate   = decay_rate\n",
    "        self.decay_steps  = tf.cast(total_steps, tf.float32)\n",
    "        self.lr_scale     = tf.cast(lr_scale, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step  = tf.cast(step, tf.float32)\n",
    "        arg1  = tf.math.rsqrt(step)\n",
    "        arg2  = step * tf.math.pow(self.warmup_steps, -1.5)\n",
    "        warm  = tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "        decay = tf.math.exp(-self.decay_rate *\n",
    "                            tf.maximum(step - self.warmup_steps, 0.) /\n",
    "                            self.decay_steps)\n",
    "        lr = warm * decay * self.lr_scale \n",
    "        return tf.maximum(lr, LR_FLOOR)\n",
    "\n",
    "    # ----------  NEW  ----------\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"d_model\":      int(self.d_model.numpy()),   # cast back to Python types\n",
    "            \"total_steps\":  int(self.decay_steps.numpy()),\n",
    "            \"warmup_frac\":  float(self.warmup_steps.numpy() / self.decay_steps.numpy()),\n",
    "            \"decay_rate\":   self.decay_rate,\n",
    "            \"lr_scale\":     float(self.lr_scale.numpy()),\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    \n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9, clipnorm=1.0)\n",
    "\n",
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(num_steps, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")\n",
    "\n",
    "print(temp_learning_rate_schedule(tf.range(num_steps, dtype=tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5501b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_for_fit = ds.map(\n",
    "    lambda b: (\n",
    "        (tf.map_fn(decode_and_resize, b['img_path'], fn_output_signature=tf.float32), b[\"input_ids\"]),\n",
    "        b[\"labels\"],\n",
    "        b[\"loss_mask\"]\n",
    "    ),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "del(ds)\n",
    "\n",
    "# 3) Compile with a standard sparseâ€CE loss and let Keras use sample weights\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=None),\n",
    "    metrics=[\"sparse_categorical_accuracy\"]\n",
    ")\n",
    "\n",
    "# Build callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"loss\", patience=3,\n",
    "                  restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint(\n",
    "        filepath=\"best_summary.keras\",        # or \"best_summary.h5\"\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=True,\n",
    "        verbose=1             # full model (weights + optimizer + LR schedule)\n",
    "    )\n",
    "]\n",
    "\n",
    "# 4) Fit!  Keras will print epoch/step progress by default\n",
    "history = model.fit(\n",
    "    ds_for_fit,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1    # 1 = progress bar, loss & acc per epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c6e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str) -> list[int]:\n",
    "    \"\"\"\n",
    "    Encodes the prompt for inference, creating a sequence that ends\n",
    "    right after the [SEP] token, with NO PADDING.\n",
    "    \"\"\"\n",
    "    # 1. Temporarily disable padding on the tokenizer\n",
    "    tokenizer.no_padding()\n",
    "    \n",
    "    # 2. Encode the text. The post-processor will add [BOS] and [SEP].\n",
    "    # This will return: [BOS, id, id, ..., SEP]\n",
    "    text_ids = tokenizer.encode(text).ids\n",
    "    \n",
    "    # 3. IMPORTANT: Re-enable padding for any other part of your code that might need it\n",
    "    tokenizer.enable_padding(pad_id=pad_id, pad_token='[PAD]', length=MAX_LEN)\n",
    "    \n",
    "    return text_ids\n",
    "\n",
    "def generate_answer(image_path: str,\n",
    "                    question:   str,\n",
    "                    max_new_tokens: int = 100) -> str:\n",
    "    \n",
    "    img_token = tf.constant([image_path])\n",
    "    img_token = tf.map_fn(decode_and_resize, img_token, fn_output_signature=tf.float32)\n",
    "    \n",
    "    # 1. Encode the prompt (image placeholders + question)\n",
    "    # This creates: [IMG]...[IMG] [BOS] question [SEP]\n",
    "    prompt_ids = encode(question)\n",
    "    text_tokens = tf.constant([prompt_ids], dtype=tf.int32)\n",
    "    \n",
    "    # 2. Autoregressive loop\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Use direct model call for performance (not model.predict)\n",
    "        # Pass training=False\n",
    "        logits = model((img_token, text_tokens), training=False)[:, -1, :] \n",
    "        \n",
    "        # Sample the next token\n",
    "        next_id = tf.random.categorical(logits, 1, dtype=tf.int32)\n",
    "\n",
    "        # Append the new token\n",
    "        text_tokens = tf.concat([text_tokens, next_id], 1)\n",
    "\n",
    "        # Stop ONLY if EOS is generated\n",
    "        if next_id[0, 0] == eos_id:\n",
    "            break\n",
    "\n",
    "    # 3. Decode the generated output\n",
    "    generated_ids = text_tokens.numpy().tolist()[0] \n",
    "    \n",
    "    # --- END FIX ---\n",
    "\n",
    "    # Display image for context\n",
    "    imgbgr = cv2.imread(image_path)\n",
    "    if imgbgr is not None:\n",
    "        imgrgb = cv2.cvtColor(imgbgr,cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(imgrgb)\n",
    "        plt.title(f\"Q: {question}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    print(tokenizer.decode(generated_ids).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279f9a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img   = \"/Users/saptarshimallikthakur/Desktop/Screenshot 2025-06-08 at 7.36.42â€¯AM.png\"\n",
    "ques  = \"what is this ?\"\n",
    "\n",
    "generate_answer(img, ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3646cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(10):\n",
    "    print('\\n',j)\n",
    "    img   = df['img_path'][j]\n",
    "    ques  = df['input_text'][j]\n",
    "    gen = generate_answer(img, ques)\n",
    "    print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2912ae40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Extraas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
