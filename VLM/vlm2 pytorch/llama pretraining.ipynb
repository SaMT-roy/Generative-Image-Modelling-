{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb2cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and split the file content\n",
    "with open('TinyStories-valid.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "tiny_stories = content.split(\"<|endoftext|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05014f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "\n",
    "# Combine summary + dialogue and split on whitespace\n",
    "raw_lens = [len(s.split()) for s in tqdm(tiny_stories, total=len(tiny_stories))]\n",
    "\n",
    "lens = np.array(raw_lens)\n",
    "\n",
    "# Print summary stats\n",
    "def pct(x): return np.percentile(lens, x)\n",
    "\n",
    "print(f\"Total examples    : {len(lens):,}\")\n",
    "print(f\"Min / Max words   : {lens.min()} / {lens.max()}\")\n",
    "print(f\"Mean ± std        : {lens.mean():.1f} ± {lens.std():.1f}\")\n",
    "print(\"--- Percentiles (word count per raw text pair) ---\")\n",
    "for p in [50, 90, 95, 98, 99]:\n",
    "    print(f\"{p:>3}% : {pct(p):.0f} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b2d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = int(pct(90))\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove HTML, normalize whitespace, preserve punctuation/numbers/casing.\n",
    "    \"\"\"\n",
    "    text = text.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "stories = []\n",
    "\n",
    "for i in tiny_stories:\n",
    "    try:\n",
    "        if len(i.strip().split()) < MAX_LEN: \n",
    "            stories.append(clean_text(i))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "import matplotlib.pyplot as plt\n",
    "        \n",
    "split = int(0.95*len(stories))\n",
    "train_texts, valid_texts = stories[:split],stories[split:]\n",
    "\n",
    "def train_tokenizer(text_iter, vocab_size: int) -> Tokenizer:\n",
    "    tok = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "    tok.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size,\n",
    "                                  min_frequency=2,\n",
    "                                  special_tokens=[\"[UNK]\"])\n",
    "    tok.train_from_iterator(text_iter, trainer)\n",
    "    return tok\n",
    "\n",
    "def avg_pieces_per_word(tok: Tokenizer, texts) -> float:\n",
    "    pieces = words = 0\n",
    "    for t in texts:\n",
    "        ids   = tok.encode(t).ids\n",
    "        pieces += len(ids)\n",
    "        words  += len(t.split())\n",
    "    return pieces / words\n",
    "\n",
    "vocab_sizes = [1000,2000,3000,4000,6000,8000]\n",
    "pieces_per_word, oov_rate = [], []\n",
    "\n",
    "for k in vocab_sizes:\n",
    "    print(k)\n",
    "    tok = train_tokenizer(train_texts, k)\n",
    "\n",
    "    # 1) average sub-words per word on validation set\n",
    "    pieces_per_word.append(avg_pieces_per_word(tok, valid_texts))\n",
    "\n",
    "    # 2) OOV percentage on validation set\n",
    "    unk_id = tok.token_to_id(\"[UNK]\")\n",
    "    total = unk = 0\n",
    "    for t in valid_texts:\n",
    "        ids   = tok.encode(t).ids\n",
    "        total += len(ids)\n",
    "        unk   += sum(id_ == unk_id for id_ in ids)\n",
    "    oov_rate.append(100 * unk / total)\n",
    "\n",
    "# ── plot ───────────────────────────────────────────────────────────────\n",
    "plt.figure(figsize=(7,3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(vocab_sizes, pieces_per_word, marker='o')\n",
    "plt.title(\"pieces / word\"); plt.xlabel(\"vocab size\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(vocab_sizes, oov_rate, marker='o')\n",
    "plt.title(\"OOV %\"); plt.xlabel(\"vocab size\")\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec7857",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 4000\n",
    "SPECIAL_TOKENS = ['[PAD]', '[UNK]', '[BOS]', '[EOS]', '[SEP]','[IMG_TOKEN]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5311cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "# Initialize and train\n",
    "tokenizer = Tokenizer(BPE(unk_token='[UNK]'))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=SPECIAL_TOKENS)\n",
    "tokenizer.train_from_iterator(train_texts, trainer=trainer)\n",
    "\n",
    "# 3) Configure post-processing, padding & truncation ------------------\n",
    "pad_id = tokenizer.token_to_id('[PAD]')\n",
    "unk_id = tokenizer.token_to_id('[UNK]')\n",
    "bos_id = tokenizer.token_to_id('[BOS]')\n",
    "eos_id = tokenizer.token_to_id('[EOS]')\n",
    "sep_id = tokenizer.token_to_id('[SEP]')\n",
    "img_token_id = tokenizer.token_to_id('[IMG_TOKEN]')\n",
    "\n",
    "# Add BOS/EOS around single sequences\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single='$A',\n",
    "    special_tokens=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846630db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 0)  Make sure `tok` points to the FINAL tokenizer\n",
    "#     → trained on the full corpus with your chosen vocab_size\n",
    "# ------------------------------------------------------------\n",
    "tok = tokenizer          # whatever variable you use\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1)  Get token counts for every example\n",
    "#     (here: concatenate summary + dialogue; split if you want)\n",
    "# ------------------------------------------------------------\n",
    "lens = []\n",
    "\n",
    "for txt in train_texts:\n",
    "    ids   = tok.encode(txt).ids\n",
    "    lens.append(len(ids))\n",
    "\n",
    "lens = np.array(lens)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2)  Print key stats\n",
    "# ------------------------------------------------------------\n",
    "def pct(x): return np.percentile(lens, x)\n",
    "\n",
    "print(f\"Total samples    : {len(lens):,}\")\n",
    "print(f\"Min / Max tokens : {lens.min()} / {lens.max()}\")\n",
    "print(f\"Mean ± std       : {lens.mean():.1f} ± {lens.std():.1f}\")\n",
    "print(\"--- percentiles (tokens) ---\")\n",
    "for p in (50, 90, 95, 98, 99):\n",
    "    print(f\"{p:>3}% : {pct(p):.0f}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3)  Quick histogram    (no seaborn, single plot, no colors set)\n",
    "# ------------------------------------------------------------\n",
    "plt.hist(lens, bins=50)\n",
    "plt.xlabel(\"tokens per example\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Tokenised length distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b97ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_LEN = int(pct(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c34c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "\n",
    "def encode_example(text: str):\n",
    "    # 1. Tokenize without padding/truncation\n",
    "    enc = tokenizer.encode(text)\n",
    "    ids = enc.ids\n",
    "\n",
    "    # 2. Add BOS token\n",
    "    ids = [bos_id] + ids\n",
    "\n",
    "    # 3. Truncate\n",
    "    if len(ids) > MAX_LEN:\n",
    "        ids = ids[:MAX_LEN]\n",
    "\n",
    "    # 4. Prepare labels\n",
    "    labels = ids[1:] + [eos_id]\n",
    "\n",
    "    # 5. Pad to MAX_LEN\n",
    "    ids = ids + [pad_id] * (MAX_LEN - len(ids))\n",
    "    labels = labels + [pad_id] * (MAX_LEN - len(labels))\n",
    "\n",
    "    # 6. Loss mask\n",
    "    loss_mask = [float(token != pad_id) for token in labels]\n",
    "\n",
    "    return torch.tensor(ids, dtype=torch.long), \\\n",
    "           torch.tensor(labels, dtype=torch.long), \\\n",
    "           torch.tensor(loss_mask, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.examples = [encode_example(t) for t in texts]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids, labels, loss_mask = self.examples[idx]\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"loss_mask\": loss_mask\n",
    "        }\n",
    "\n",
    "dataset = TextDataset(train_texts)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f01e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()           # indices 0 … V\n",
    "\n",
    "id_to_token = {idx: tok for tok, idx in vocab.items()}\n",
    "\n",
    "# 2) Decoder: drop PADs, replace unknown IDs with '?'\n",
    "def decode_token_ids(token_ids: list[int]) -> str:\n",
    "    tokens = []\n",
    "    for tid in token_ids:\n",
    "        if tid == pad_id:\n",
    "            continue\n",
    "        tokens.append(id_to_token.get(tid, '?'))\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Take one batch\n",
    "batch = next(iter(loader))\n",
    "\n",
    "input_ids = batch['input_ids']   # shape: [B, MAX_LEN]\n",
    "labels    = batch['labels']      # shape: [B, MAX_LEN]\n",
    "\n",
    "for i in range(input_ids.size(0)):\n",
    "    ids_row = input_ids[i].tolist()\n",
    "    lbl_row = labels[i].tolist()\n",
    "\n",
    "    print(f\"\\n🟢 Sample {i+1}\")\n",
    "    print(\"  Input IDs: \", ids_row)\n",
    "    print(\"  Decoded:   \", decode_token_ids(ids_row))  # Assumes you have this function defined\n",
    "    print(\"  Label IDs: \", lbl_row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e275e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "j=1\n",
    "print(batch[\"loss_mask\"][j].numpy())\n",
    "print('\\n',decode_token_ids(batch[\"labels\"][j].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197c6c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "\n",
    "class BuildTransformer(nn.Module):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, input_vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding = TokenEmbedding(vocab_size=input_vocab_size, d_model=d_model)\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads,\n",
    "                               input_vocab_size=input_vocab_size, dropout_rate=dropout_rate)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        # Create padding mask (True for padding tokens, False otherwise)\n",
    "        pad_mask = (x == 0)  # Assuming padding token is 0, as in TokenEmbedding's padding_idx\n",
    "        x = self.token_embedding(x)\n",
    "        x = self.decoder(x, pad_mask=pad_mask)\n",
    "        return x\n",
    "\n",
    "# Example instantiation\n",
    "num_layers = 2\n",
    "d_model = 256\n",
    "num_heads = 8  # 32 dim per head\n",
    "dropout_rate = 0.1\n",
    "VOCAB_SIZE = 4000  # Example value, replace with actual VOCAB_SIZE\n",
    "MAX_LEN = 180      # Example value, replace with actual MAX_LEN\n",
    "\n",
    "model = BuildTransformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    input_vocab_size=VOCAB_SIZE,\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "\n",
    "# Build weights by running a forward pass\n",
    "dummy = torch.zeros((1, MAX_LEN), dtype=torch.int64)\n",
    "_ = model(dummy)\n",
    "\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name.lower():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Model summary\n",
    "summary(model, input_size=(1, MAX_LEN), dtypes=[torch.int64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372c2a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trainable Parameters:\")\n",
    "print(\"-\" * 60)\n",
    "total_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        param_count = param.numel()\n",
    "        print(f\"Parameter: {name:<50} Shape: {str(param.shape):<20} Parameters: {param_count}\")\n",
    "        total_params += param_count\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total Trainable Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b6ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Non Trainable Parameters:\")\n",
    "print(\"-\" * 60)\n",
    "total_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        param_count = param.numel()\n",
    "        print(f\"Parameter: {name:<50} Shape: {str(param.shape):<20} Parameters: {param_count}\")\n",
    "        total_params += param_count\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total Non Trainable Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfa38cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": input_ids,     # shape [MAX_LEN]\n",
    "            \"labels\": labels,           # shape [MAX_LEN]\n",
    "            \"loss_mask\": loss_mask      # shape [MAX_LEN], float mask\n",
    "        }\n",
    "    \n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def masked_sparse_crossentropy(logits, target, mask):\n",
    "    \"\"\"\n",
    "    logits: (B, T, V)\n",
    "    target: (B, T)\n",
    "    mask:   (B, T) — float32 (1.0 = include, 0.0 = ignore)\n",
    "    \"\"\"\n",
    "    loss = F.cross_entropy(\n",
    "        logits.transpose(1, 2),  # shape: (B, V, T)\n",
    "        target,\n",
    "        reduction='none'         # we’ll apply masking manually\n",
    "    )  # shape: (B, T)\n",
    "\n",
    "    loss = loss * mask  # apply mask\n",
    "    return loss.sum() / mask.sum()  # normalize over actual tokens\n",
    "\n",
    "def masked_accuracy(logits, labels, mask):\n",
    "    preds = logits.argmax(dim=-1)  # shape: (B, T)\n",
    "    correct = (preds == labels).float() * mask\n",
    "    return correct.sum() / mask.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cpu'\n",
    "EPOCHS = 20\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Wrap loader with tqdm for batch-wise progress\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
    "    \n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)    # (B, T)\n",
    "        labels    = batch['labels'].to(device)       # (B, T)\n",
    "        mask      = batch['loss_mask'].to(device)    # (B, T)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)  # expected output shape: (B, T, V)\n",
    "\n",
    "        loss = masked_sparse_crossentropy(logits, labels, mask)\n",
    "        acc  = masked_accuracy(logits, labels, mask)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        num_tokens = mask.sum().item()\n",
    "        total_loss += loss.item() * num_tokens\n",
    "        total_acc  += acc.item()  * num_tokens\n",
    "        total_tokens += num_tokens\n",
    "\n",
    "        # Show live batch metrics in tqdm bar\n",
    "        avg_loss = total_loss / total_tokens\n",
    "        avg_acc = total_acc / total_tokens\n",
    "        pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"acc\": f\"{avg_acc:.4f}\"})\n",
    "\n",
    "    print(f\"✅ Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f} | Accuracy: {avg_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(text: str, max_len: int = MAX_LEN) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns [BOS] dialogue [EOS] *without* any padding.\n",
    "    \"\"\"\n",
    "    tokenizer.no_padding()\n",
    "    tokenizer.enable_truncation(max_length=max_len)\n",
    "    ids = [bos_id] + tokenizer.encode(clean_text(text)).ids\n",
    "    return torch.tensor(ids, dtype=torch.long)  # 1-D tensor\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_summary(\n",
    "    text_ids: torch.Tensor,\n",
    "    model: torch.nn.Module,\n",
    "    max_new: int = 30,\n",
    "    temperature: float = 1.0,\n",
    "    device: str = \"cpu\"\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate tokens auto-regressively: [BOS] text [EOS]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    prompt = text_ids.unsqueeze(0).to(device)  # (1, T)\n",
    "\n",
    "    for _ in range(max_new):\n",
    "        if prompt.shape[1] > MAX_LEN:\n",
    "            break\n",
    "\n",
    "        logits = model(prompt)                 # (1, T, V)\n",
    "        next_logits = logits[:, -1, :] / temperature  # (1, V)\n",
    "\n",
    "        probs = torch.softmax(next_logits, dim=-1)     # (1, V)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)  # (1, 1)\n",
    "\n",
    "        prompt = torch.cat([prompt, next_id], dim=1)   # append\n",
    "\n",
    "        if next_id.item() == eos_id:\n",
    "            break\n",
    "\n",
    "    return prompt.squeeze(0).cpu()\n",
    "\n",
    "def display_text(text: str, model, device=\"cpu\"):\n",
    "    # 1. Encode input\n",
    "    text_ids = encode_input(text)\n",
    "\n",
    "    # 2. Generate full sequence\n",
    "    full_ids = generate_summary(text_ids, model, device=device)\n",
    "\n",
    "    # 3. Convert to list\n",
    "    full_ids_list = full_ids.tolist()\n",
    "\n",
    "    # 4. Decode\n",
    "    text_str = decode_token_ids(full_ids_list)\n",
    "\n",
    "    # 5. Print\n",
    "    print(\"\\n Text:\", text_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35c8036",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_text('milk in kitchen', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1292b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer.json\")\n",
    "\n",
    "torch.save(model.state_dict(), 'build_transformer_trained_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479d100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\"token_embedding\": model.token_embedding.state_dict()},\n",
    "    \"embedding_layer_dict.pth\"\n",
    ")\n",
    "\n",
    "torch.save(\n",
    "    {\"decoder\": model.decoder.state_dict()},\n",
    "    \"decoder_only_weights.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb8ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef2062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ff898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f40690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding layer\n",
    "enc = TokenEmbedding(vocab_size=4000, d_model=256)\n",
    "state = torch.load(\"embedding_layer_dict.pth\")\n",
    "enc.load_state_dict(state[\"token_embedding\"])\n",
    "enc.eval()  # disable dropout if any\n",
    "\n",
    "# Example token(s)\n",
    "token_id = torch.tensor([42])        # single token → shape (1,)\n",
    "token_ids = torch.tensor([[42, 88]]) # token sequence → shape (1, T)\n",
    "\n",
    "# Generate embedding\n",
    "with torch.no_grad():\n",
    "    emb_single = enc(token_id)       # shape: (1, d_model)\n",
    "    emb_seq = enc(token_ids)         # shape: (1, T, d_model)\n",
    "\n",
    "print(\"Single Token Embedding:\", emb_single.shape)\n",
    "print(\"Token Sequence Embedding:\", emb_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd31923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild decoder (must match original architecture)\n",
    "decoder = Decoder(\n",
    "    num_layers=2,\n",
    "    d_model=256,\n",
    "    num_heads=8,\n",
    "    input_vocab_size=4000,  # should match vocab used in final layer\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "decoder.load_state_dict(torch.load(\"decoder_only_weights.pth\")[\"decoder\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a8849b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Extraas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
