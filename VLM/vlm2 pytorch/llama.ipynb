{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "886ea2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import *\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd784aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "BuildTransformer                                   [1, 180, 4000]            --\n",
       "├─TokenEmbedding: 1-1                              [1, 180, 256]             --\n",
       "│    └─Embedding: 2-1                              [1, 180, 256]             1,024,000\n",
       "├─Decoder: 1-2                                     [1, 180, 4000]            --\n",
       "│    └─Dropout: 2-2                                [1, 180, 256]             --\n",
       "│    └─ModuleList: 2-3                             --                        --\n",
       "│    │    └─DecoderLayer: 3-1                      [1, 180, 256]             674,304\n",
       "│    │    └─DecoderLayer: 3-2                      [1, 180, 256]             674,304\n",
       "│    └─RMSNorm: 2-4                                [1, 180, 256]             256\n",
       "│    └─Linear: 2-5                                 [1, 180, 4000]            1,028,000\n",
       "====================================================================================================\n",
       "Total params: 3,400,864\n",
       "Trainable params: 3,400,864\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 3.40\n",
       "====================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 19.10\n",
       "Params size (MB): 13.60\n",
       "Estimated Total Size (MB): 32.71\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BuildTransformer(nn.Module):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, input_vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding = TokenEmbedding(vocab_size=input_vocab_size, d_model=d_model)\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads,\n",
    "                               input_vocab_size=input_vocab_size, dropout_rate=dropout_rate)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        # Create padding mask (True for padding tokens, False otherwise)\n",
    "        pad_mask = (x == 0)  # Assuming padding token is 0, as in TokenEmbedding's padding_idx\n",
    "        x = self.token_embedding(x)\n",
    "        x = self.decoder(x, pad_mask=pad_mask)\n",
    "        return x\n",
    "\n",
    "# Example instantiation\n",
    "num_layers = 2\n",
    "d_model = 256\n",
    "num_heads = 8  # 32 dim per head\n",
    "dropout_rate = 0.1\n",
    "VOCAB_SIZE = 4000  # Example value, replace with actual VOCAB_SIZE\n",
    "MAX_LEN = 180      # Example value, replace with actual MAX_LEN\n",
    "\n",
    "model = BuildTransformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    input_vocab_size=VOCAB_SIZE,\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "\n",
    "# Build weights by running a forward pass\n",
    "dummy = torch.zeros((1, MAX_LEN), dtype=torch.int64)\n",
    "_ = model(dummy)\n",
    "\n",
    "# Model summary\n",
    "summary(model, input_size=(1, MAX_LEN), dtypes=[torch.int64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c71483af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters:\n",
      "------------------------------------------------------------\n",
      "Parameter: token_embedding.embedding.weight                   Shape: torch.Size([4000, 256]) Parameters: 1024000\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.mha.wq.weight Shape: torch.Size([256, 256]) Parameters: 65536\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.mha.wk.weight Shape: torch.Size([256, 256]) Parameters: 65536\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.mha.wv.weight Shape: torch.Size([256, 256]) Parameters: 65536\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.mha.wo.weight Shape: torch.Size([256, 256]) Parameters: 65536\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.mha.lora_wq.A.weight Shape: torch.Size([8, 256]) Parameters: 2048\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.mha.lora_wq.B.weight Shape: torch.Size([256, 8]) Parameters: 2048\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.mha.lora_wv.A.weight Shape: torch.Size([8, 256]) Parameters: 2048\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.mha.lora_wv.B.weight Shape: torch.Size([256, 8]) Parameters: 2048\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.rmsnorm.scale Shape: torch.Size([256])    Parameters: 256\n",
      "Parameter: decoder.dec_layers.0.ffn.swiglu.lin1.weight        Shape: torch.Size([1024, 256]) Parameters: 262144\n",
      "Parameter: decoder.dec_layers.0.ffn.swiglu.lin2.weight        Shape: torch.Size([256, 512]) Parameters: 131072\n",
      "Parameter: decoder.dec_layers.0.ffn.swiglu.lora_lin1.A.weight Shape: torch.Size([8, 256]) Parameters: 2048\n",
      "Parameter: decoder.dec_layers.0.ffn.swiglu.lora_lin1.B.weight Shape: torch.Size([1024, 8]) Parameters: 8192\n",
      "Parameter: decoder.dec_layers.0.ffn.rmsnorm.scale             Shape: torch.Size([256])    Parameters: 256\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.mha.wq.weight Shape: torch.Size([256, 256]) Parameters: 65536\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.mha.wk.weight Shape: torch.Size([256, 256]) Parameters: 65536\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.mha.wv.weight Shape: torch.Size([256, 256]) Parameters: 65536\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.mha.wo.weight Shape: torch.Size([256, 256]) Parameters: 65536\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.mha.lora_wq.A.weight Shape: torch.Size([8, 256]) Parameters: 2048\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.mha.lora_wq.B.weight Shape: torch.Size([256, 8]) Parameters: 2048\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.mha.lora_wv.A.weight Shape: torch.Size([8, 256]) Parameters: 2048\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.mha.lora_wv.B.weight Shape: torch.Size([256, 8]) Parameters: 2048\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.rmsnorm.scale Shape: torch.Size([256])    Parameters: 256\n",
      "Parameter: decoder.dec_layers.1.ffn.swiglu.lin1.weight        Shape: torch.Size([1024, 256]) Parameters: 262144\n",
      "Parameter: decoder.dec_layers.1.ffn.swiglu.lin2.weight        Shape: torch.Size([256, 512]) Parameters: 131072\n",
      "Parameter: decoder.dec_layers.1.ffn.swiglu.lora_lin1.A.weight Shape: torch.Size([8, 256]) Parameters: 2048\n",
      "Parameter: decoder.dec_layers.1.ffn.swiglu.lora_lin1.B.weight Shape: torch.Size([1024, 8]) Parameters: 8192\n",
      "Parameter: decoder.dec_layers.1.ffn.rmsnorm.scale             Shape: torch.Size([256])    Parameters: 256\n",
      "Parameter: decoder.rmsnorm.scale                              Shape: torch.Size([256])    Parameters: 256\n",
      "Parameter: decoder.final_layer.weight                         Shape: torch.Size([4000, 256]) Parameters: 1024000\n",
      "Parameter: decoder.final_layer.bias                           Shape: torch.Size([4000])   Parameters: 4000\n",
      "------------------------------------------------------------\n",
      "Total Trainable Parameters: 3,400,864\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainable Parameters:\")\n",
    "print(\"-\" * 60)\n",
    "total_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        param_count = param.numel()\n",
    "        print(f\"Parameter: {name:<50} Shape: {str(param.shape):<20} Parameters: {param_count}\")\n",
    "        total_params += param_count\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total Trainable Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3b4b81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters:\n",
      "------------------------------------------------------------\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.mha.lora_wq.A.weight Shape: torch.Size([8, 256]) Parameters: 2048\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.mha.lora_wq.B.weight Shape: torch.Size([256, 8]) Parameters: 2048\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.mha.lora_wv.A.weight Shape: torch.Size([8, 256]) Parameters: 2048\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.mha.lora_wv.B.weight Shape: torch.Size([256, 8]) Parameters: 2048\n",
      "Parameter: decoder.dec_layers.0.ffn.swiglu.lora_lin1.A.weight Shape: torch.Size([8, 256]) Parameters: 2048\n",
      "Parameter: decoder.dec_layers.0.ffn.swiglu.lora_lin1.B.weight Shape: torch.Size([1024, 8]) Parameters: 8192\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.mha.lora_wq.A.weight Shape: torch.Size([8, 256]) Parameters: 2048\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.mha.lora_wq.B.weight Shape: torch.Size([256, 8]) Parameters: 2048\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.mha.lora_wv.A.weight Shape: torch.Size([8, 256]) Parameters: 2048\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.mha.lora_wv.B.weight Shape: torch.Size([256, 8]) Parameters: 2048\n",
      "Parameter: decoder.dec_layers.1.ffn.swiglu.lora_lin1.A.weight Shape: torch.Size([8, 256]) Parameters: 2048\n",
      "Parameter: decoder.dec_layers.1.ffn.swiglu.lora_lin1.B.weight Shape: torch.Size([1024, 8]) Parameters: 8192\n",
      "------------------------------------------------------------\n",
      "Total Trainable Parameters: 36,864\n"
     ]
    }
   ],
   "source": [
    "# Freeze all parameters except those in LoraLayer\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' not in name.lower():\n",
    "        param.requires_grad = False\n",
    "\n",
    "print(\"Trainable Parameters:\")\n",
    "print(\"-\" * 60)\n",
    "total_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        param_count = param.numel()\n",
    "        print(f\"Parameter: {name:<50} Shape: {str(param.shape):<20} Parameters: {param_count}\")\n",
    "        total_params += param_count\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total Trainable Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76065f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Gradients (LoRA should have gradients, others should not):\n",
      "--------------------------------------------------------------------------------\n",
      "Parameter: token_embedding.embedding.weight                             Shape: torch.Size([4000, 256]) Trainable: 0          No Gradient\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.mha.wq.weight     Shape: torch.Size([256, 256]) Trainable: 0          No Gradient\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.mha.wk.weight     Shape: torch.Size([256, 256]) Trainable: 0          No Gradient\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.mha.wv.weight     Shape: torch.Size([256, 256]) Trainable: 0          No Gradient\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.mha.wo.weight     Shape: torch.Size([256, 256]) Trainable: 0          No Gradient\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.mha.lora_wq.A.weight Shape: torch.Size([8, 256]) Trainable: 1          Has Gradient\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.mha.lora_wq.B.weight Shape: torch.Size([256, 8]) Trainable: 1          Has Gradient\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.mha.lora_wv.A.weight Shape: torch.Size([8, 256]) Trainable: 1          Has Gradient\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.mha.lora_wv.B.weight Shape: torch.Size([256, 8]) Trainable: 1          Has Gradient\n",
      "Parameter: decoder.dec_layers.0.causal_self_attention.rmsnorm.scale     Shape: torch.Size([256])    Trainable: 0          No Gradient\n",
      "Parameter: decoder.dec_layers.0.ffn.swiglu.lin1.weight                  Shape: torch.Size([1024, 256]) Trainable: 0          No Gradient\n",
      "Parameter: decoder.dec_layers.0.ffn.swiglu.lin2.weight                  Shape: torch.Size([256, 512]) Trainable: 0          No Gradient\n",
      "Parameter: decoder.dec_layers.0.ffn.swiglu.lora_lin1.A.weight           Shape: torch.Size([8, 256]) Trainable: 1          Has Gradient\n",
      "Parameter: decoder.dec_layers.0.ffn.swiglu.lora_lin1.B.weight           Shape: torch.Size([1024, 8]) Trainable: 1          Has Gradient\n",
      "Parameter: decoder.dec_layers.0.ffn.rmsnorm.scale                       Shape: torch.Size([256])    Trainable: 0          No Gradient\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.mha.wq.weight     Shape: torch.Size([256, 256]) Trainable: 0          No Gradient\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.mha.wk.weight     Shape: torch.Size([256, 256]) Trainable: 0          No Gradient\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.mha.wv.weight     Shape: torch.Size([256, 256]) Trainable: 0          No Gradient\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.mha.wo.weight     Shape: torch.Size([256, 256]) Trainable: 0          No Gradient\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.mha.lora_wq.A.weight Shape: torch.Size([8, 256]) Trainable: 1          Has Gradient\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.mha.lora_wq.B.weight Shape: torch.Size([256, 8]) Trainable: 1          Has Gradient\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.mha.lora_wv.A.weight Shape: torch.Size([8, 256]) Trainable: 1          Has Gradient\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.mha.lora_wv.B.weight Shape: torch.Size([256, 8]) Trainable: 1          Has Gradient\n",
      "Parameter: decoder.dec_layers.1.causal_self_attention.rmsnorm.scale     Shape: torch.Size([256])    Trainable: 0          No Gradient\n",
      "Parameter: decoder.dec_layers.1.ffn.swiglu.lin1.weight                  Shape: torch.Size([1024, 256]) Trainable: 0          No Gradient\n",
      "Parameter: decoder.dec_layers.1.ffn.swiglu.lin2.weight                  Shape: torch.Size([256, 512]) Trainable: 0          No Gradient\n",
      "Parameter: decoder.dec_layers.1.ffn.swiglu.lora_lin1.A.weight           Shape: torch.Size([8, 256]) Trainable: 1          Has Gradient\n",
      "Parameter: decoder.dec_layers.1.ffn.swiglu.lora_lin1.B.weight           Shape: torch.Size([1024, 8]) Trainable: 1          Has Gradient\n",
      "Parameter: decoder.dec_layers.1.ffn.rmsnorm.scale                       Shape: torch.Size([256])    Trainable: 0          No Gradient\n",
      "Parameter: decoder.rmsnorm.scale                                        Shape: torch.Size([256])    Trainable: 0          No Gradient\n",
      "Parameter: decoder.final_layer.weight                                   Shape: torch.Size([4000, 256]) Trainable: 0          No Gradient\n",
      "Parameter: decoder.final_layer.bias                                     Shape: torch.Size([4000])   Trainable: 0          No Gradient\n",
      "--------------------------------------------------------------------------------\n",
      "Total Trainable Parameters: 36,864\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy input and target for gradient computation\n",
    "dummy_input = torch.zeros((1, MAX_LEN), dtype=torch.int64)\n",
    "dummy_target = torch.zeros((1, MAX_LEN), dtype=torch.int64)  # Dummy target for loss\n",
    "\n",
    "# Ensure model is in training mode\n",
    "model.train()\n",
    "\n",
    "# Forward pass\n",
    "output = model(dummy_input)  # Shape: [1, 512, 10000]\n",
    "\n",
    "# Compute a dummy loss (e.g., cross-entropy loss)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(output.view(-1, VOCAB_SIZE), dummy_target.view(-1))\n",
    "\n",
    "# Backward pass to compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Check gradients for all parameters\n",
    "print(\"Parameter Gradients (LoRA should have gradients, others should not):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "total_trainable_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    grad_status = \"Has Gradient\" if param.grad is not None else \"No Gradient\"\n",
    "    param_count = param.numel()\n",
    "    if param.requires_grad:\n",
    "        total_trainable_params += param_count\n",
    "    print(f\"Parameter: {name:<60} Shape: {str(param.shape):<20} Trainable: {param.requires_grad:<10} {grad_status}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total Trainable Parameters: {total_trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e840164d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Extraas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
