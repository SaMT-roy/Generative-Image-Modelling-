architectures = [    "LlamaForCausalLM"  ]
attention_bias = False
attention_dropout = 0.0
bos_token_id = 1
eos_token_id = 2
hidden_act = "silu"
hidden_size = 576
initializer_range = 0.041666666666666664
intermediate_size = 1536
is_llama_config = True
max_position_embeddings = 8192
mlp_bias = False
model_type = "llama"
num_attention_heads = 9
num_hidden_layers = 30
num_key_value_heads = 3
pad_token_id = 2
pretraining_tp = 1
rms_norm_eps = 1e-05
rope_interleaved = False
rope_scaling = None
rope_theta = 100000
tie_word_embeddings = True
torch_dtype = "bfloat16"
transformers_version = "4.42.3"
use_cache = True
vocab_size = 49152